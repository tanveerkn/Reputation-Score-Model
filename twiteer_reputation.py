# -*- coding: utf-8 -*-
"""twiteer reputation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AWACZupd17G92Hx4hIIUwVVoYKMXDu_C
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import seaborn as sb
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn import linear_model
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
import numpy as np
from keras.callbacks import ModelCheckpoint
from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler

file=('dataframealluser1.csv')
features = pd.read_csv(file)
corr = features.corr()
print("Correlation of features with the reputation score \n")
print (corr['Reputation_score'])
features=features.iloc[ : ,3:]

# X=features.iloc[:,:-1]
# X=X.clip(features.quantile(0.20), features.quantile(0.80), axis=1)
# y=features.iloc[:,-1].values
# col = [x for x in list(X) if x not in ['id_str', 'screen_name','Social_reputation', 'like_hindex','retweet_hindex', 'Content_Score', 'Context_score']]
# X=X[col].values

# model = []
# accuracy = []
# error= []
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# sc_X=StandardScaler()
# X_train=sc_X.fit_transform(X_train)
# X_test=sc_X.transform(X_test)

# sc_y=StandardScaler()
# y_train=y_train.reshape (-1,1)
# y_test=y_test.reshape (-1,1)
# y_train=sc_y.fit_transform(y_train)
# # y_train = y_train.reshape (-1,1)
# y_test=sc_y.transform(y_test)
# # y_test = y_test.reshape (-1,1)
# y_test

def make_train_test_set(df, train_test_split_prct, clipping_quantile):
    msk = np.random.rand(len(df)) < train_test_split_prct
    train_df = df[msk].copy()

    test_df = df[~msk].copy()


    thres = train_df.quantile(clipping_quantile)
    # test_qu=train_df.quantile(clipping_quantile)


    fet_list = [x for x in list(df) if x not in ['id_str', 'screen_name',"symbols_ratio"]]

    for col in fet_list:

        if col:
            train_df[col] = train_df[col] / thres[col]

            test_df[col] = test_df[col] / thres[col]

    train_df.iloc[:,2:-1].clip(0,1)

    test_df.iloc[:,2:-1].clip(0,1)

    # , 'statuses_count', 'followers_count', 'listed_count', 'friends_count', 'has_url', 'mention_by_others', 'retweet_ratio', 'liked_ratio', 'orig_content_ratio', 'hashtag_ratio', 'urls_ratio', 'symbols_ratio', 'mentions_ratio', 'Social_reputation', 'retweet_hindex', 'like_hindex', 'Content_Score', 'Context_score']]

    cols = [col for col in list(df) if col not in ['id_str', 'screen_name','Social_reputation', 'retweet_hindex', 'Content_Score', 'Context_score']]

    y_train = train_df['Reputation_score'].values
    # print(y_train)

    y_test = test_df['Reputation_score'].values
    # print(y_test)
    X_train = train_df[cols].values

    X_test = test_df[cols].values

    return X_train, X_test, y_train, y_test, thres.transpose(), fet_list



X_train, X_test, y_train, y_test, thres, fet_list = make_train_test_set(features, 0.8 , 0.9)



print("Comparision of Models ....\n")

  print("\n Model Name \t\t\t MSE_REG \n")
  for key in regr_dict.keys():
      print("{0:20}\t\t {1:.5f}\t\t{2:.5f}\n".format(key, MSE_reg[key],Var_reg[key]))

"""# Multi Layer Perceptron"""

from matplotlib import pyplot
svr = MLPRegressor(
    hidden_layer_sizes=(50,),  activation='relu', solver='adam', alpha=0.001, batch_size='auto',
    learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=100, shuffle=True,
    random_state=9, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,
    early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)


svr_dict  = {"Poly": SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1, coef0=1),
              "Linear": SVR(kernel='linear', C=100, gamma='auto'),
              "RBF": SVR(kernel='rbf', C=100, gamma='auto', degree=3, epsilon=.1, coef0=1)
              }

# mse_svr_list = []
# lets run the experiment multiple times
MSE_svr = {}
Var_svr = {}
MAE_svr={}
RMSE={}

# #
for key in svr_dict.keys():
  print(key)
  svr = svr_dict[key]
  print(svr)
  # Train the model using the training sets
  history=svr.fit(X_train, y_train)
#   print(svr.summary())
  y_pred_svr = svr.predict(X_test)
  model.append(key)

  MSE_svr[key] = metrics.mean_squared_error(y_test, y_pred_svr)
  MAE_svr[key]=metrics.mean_absolute_error(y_test, y_pred_svr)
  RMSE[key]=np.sqrt(metrics.mean_squared_error(y_test, y_pred_svr))
  print("hello2")
  Var_svr[key] = svr.score(X_test, y_test)
  accuracy.append(Var_svr[key])
  error.append(MSE_svr[key])
  print("hello3")
  df = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred_svr.flatten()})
  df1 = df
  df1.plot(kind='bar', figsize=(16, 10))
  plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
  plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
  plt.ylabel('Error Difference')
  plt.xlabel('Number of Test Samples')
  plt.title('{}'.format(key))
  # plt.scatter(X_test, y_test, color='gray')
  # plt.plot(X_test, y_pred_svr, color='red', linewidth=2)
  plt.savefig(key+'.png', dpi=300, bbox_inches='tight')
  plt.show()
  
#   pyplot.title('Loss / Mean Squared Error')
#   pyplot.plot(history.history['loss'], label='train')
#   pyplot.plot(history.history['val_loss'], label='test')
#   pyplot.legend()
#   pyplot.show()
  print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_svr))
  print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_svr))
  print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_svr)))
  print("Comparision of Models ....\n")

print("\n Model Name \t\t\t MSE_MLP \n")
for key in svr_dict.keys():
  print("{0:20}\t\t {1:.5f}\t\t{2:.5f}\n".format(key, MSE_svr[key],Var_svr[key]))

social media and machine learning conferences 2020

ls

# ######################Deep Neural network for regressor################
# # Define a sequential model
# # Add some dense layers
# # Use ‘relu’ as the activation function for the hidden layers
# # Use a ‘normal’ initializer as the kernal_intializer
# # We will use mean_absolute_error as a loss function
# # Define the output layer with only one node
# # Use ‘linear ’as the activation function for the output layer
#
NN_model = Sequential()

# The Input Layer :
NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))

# The Hidden Layers :
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))

# The Output Layer :
NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))

# Compile the network :
NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])
NN_model.summary()


########Define a check point####################
checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5'
checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')
callbacks_list = [checkpoint]

history=NN_model.fit(X_train, y_train, epochs=1000, batch_size=32, validation_data=(X_test,y_test), callbacks=callbacks_list)
print(len(NN_model.predict(X_test)))
# print(NN_model.history.keys())
# "Loss"
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Mean Squared Error')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.savefig('MSEDNN.png', dpi=300, bbox_inches='tight')
plt.show()
# plt.savefig('mse.png')


# ##########MLP regressor#########################




############################################################################################

ls

#####################Plotting graph for socre of different models###############################


index = np.arange(len(model))
plt.bar(index,accuracy,alpha=1,color='blue')
plt.ylabel('Accuracy')
plt.xlabel('Different Machine Learning Models')
plt.xticks(index,model)
plt.title("Models Comparison")
plt.show()
#######feautre correlation visualizing using seaborn###############