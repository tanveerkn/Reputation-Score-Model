{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twiteer reputation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanveerkn/Reputation-Score-Model/blob/master/twiteer_reputation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NccjsWaXvbEN",
        "colab_type": "code",
        "outputId": "4fd621d4-87e5-4a67-daba-8e2fc9ece259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn import linear_model\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.svm import SVR\n",
        "import numpy as np\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf2B6K5DZ6J4",
        "colab_type": "code",
        "outputId": "0090ced8-bf33-498e-87bb-0d4544d77027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "file=('dataframealluser1.csv')\n",
        "features = pd.read_csv(file)\n",
        "corr = features.corr()\n",
        "print(\"Correlation of features with the reputation score \\n\")\n",
        "print (corr['Reputation_score'])\n",
        "features=features.iloc[ : ,3:]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correlation of features with the reputation score \n",
            "\n",
            "id_str               -0.204611\n",
            "statuses_count        0.073880\n",
            "followers_count       0.636434\n",
            "listed_count          0.433960\n",
            "friends_count         0.030450\n",
            "has_url               0.122642\n",
            "mention_by_others     0.336129\n",
            "retweet_ratio        -0.112384\n",
            "liked_ratio           0.222847\n",
            "orig_content_ratio    0.107863\n",
            "hashtag_ratio        -0.086178\n",
            "urls_ratio            0.003500\n",
            "symbols_ratio         0.135827\n",
            "mentions_ratio       -0.099517\n",
            "Social_reputation     0.662557\n",
            "retweet_hindex       -0.131747\n",
            "like_hindex           0.999845\n",
            "Content_Score        -0.039494\n",
            "Context_score         0.106145\n",
            "Reputation_score      1.000000\n",
            "Name: Reputation_score, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmG6GOqfvI02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X=features.iloc[:,:-1]\n",
        "# X=X.clip(features.quantile(0.20), features.quantile(0.80), axis=1)\n",
        "# y=features.iloc[:,-1].values\n",
        "# col = [x for x in list(X) if x not in ['id_str', 'screen_name','Social_reputation', 'like_hindex','retweet_hindex', 'Content_Score', 'Context_score']]\n",
        "# X=X[col].values\n",
        "\n",
        "# model = []\n",
        "# accuracy = []\n",
        "# error= []\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# sc_X=StandardScaler()\n",
        "# X_train=sc_X.fit_transform(X_train)\n",
        "# X_test=sc_X.transform(X_test)\n",
        "\n",
        "# sc_y=StandardScaler()\n",
        "# y_train=y_train.reshape (-1,1)\n",
        "# y_test=y_test.reshape (-1,1)\n",
        "# y_train=sc_y.fit_transform(y_train)\n",
        "# # y_train = y_train.reshape (-1,1)\n",
        "# y_test=sc_y.transform(y_test)\n",
        "# # y_test = y_test.reshape (-1,1)\n",
        "# y_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk-G95czOwX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_train_test_set(df, train_test_split_prct, clipping_quantile):\n",
        "    msk = np.random.rand(len(df)) < train_test_split_prct\n",
        "    train_df = df[msk].copy()\n",
        "\n",
        "    test_df = df[~msk].copy()\n",
        "\n",
        "\n",
        "    thres = train_df.quantile(clipping_quantile)\n",
        "    # test_qu=train_df.quantile(clipping_quantile)\n",
        "\n",
        "\n",
        "    fet_list = [x for x in list(df) if x not in ['id_str', 'screen_name',\"symbols_ratio\"]]\n",
        "\n",
        "    for col in fet_list:\n",
        "\n",
        "        if col:\n",
        "            train_df[col] = train_df[col] / thres[col]\n",
        "\n",
        "            test_df[col] = test_df[col] / thres[col]\n",
        "\n",
        "    train_df.iloc[:,2:-1].clip(0,1)\n",
        "\n",
        "    test_df.iloc[:,2:-1].clip(0,1)\n",
        "\n",
        "    # , 'statuses_count', 'followers_count', 'listed_count', 'friends_count', 'has_url', 'mention_by_others', 'retweet_ratio', 'liked_ratio', 'orig_content_ratio', 'hashtag_ratio', 'urls_ratio', 'symbols_ratio', 'mentions_ratio', 'Social_reputation', 'retweet_hindex', 'like_hindex', 'Content_Score', 'Context_score']]\n",
        "\n",
        "    cols = [col for col in list(df) if col not in ['id_str', 'screen_name','Social_reputation', 'retweet_hindex', 'Content_Score', 'Context_score']]\n",
        "\n",
        "    y_train = train_df['Reputation_score'].values\n",
        "    # print(y_train)\n",
        "\n",
        "    y_test = test_df['Reputation_score'].values\n",
        "    # print(y_test)\n",
        "    X_train = train_df[cols].values\n",
        "\n",
        "    X_test = test_df[cols].values\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, thres.transpose(), fet_list\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test, thres, fet_list = make_train_test_set(features, 0.8 , 0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UARemlWV80gh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj7FRnjKJmHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  print(\"Comparision of Models ....\\n\")\n",
        "\n",
        "  print(\"\\n Model Name \\t\\t\\t MSE_REG \\n\")\n",
        "  for key in regr_dict.keys():\n",
        "      print(\"{0:20}\\t\\t {1:.5f}\\t\\t{2:.5f}\\n\".format(key, MSE_reg[key],Var_reg[key]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7l5KQKtJcxH",
        "colab_type": "text"
      },
      "source": [
        "# Multi Layer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE0UuW91Ee7d",
        "colab_type": "code",
        "outputId": "994769dd-2e44-40b1-8aa1-24a4470c9c1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from matplotlib import pyplot\n",
        "svr = MLPRegressor(\n",
        "    hidden_layer_sizes=(50,),  activation='relu', solver='adam', alpha=0.001, batch_size='auto',\n",
        "    learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=100, shuffle=True,\n",
        "    random_state=9, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
        "    early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "\n",
        "\n",
        "svr_dict  = {\"Poly\": SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1, coef0=1),\n",
        "              \"Linear\": SVR(kernel='linear', C=100, gamma='auto'),\n",
        "              \"RBF\": SVR(kernel='rbf', C=100, gamma='auto', degree=3, epsilon=.1, coef0=1)\n",
        "              }\n",
        "\n",
        "# mse_svr_list = []\n",
        "# lets run the experiment multiple times\n",
        "MSE_svr = {}\n",
        "Var_svr = {}\n",
        "MAE_svr={}\n",
        "RMSE={}\n",
        "\n",
        "# #\n",
        "for key in svr_dict.keys():\n",
        "  print(key)\n",
        "  svr = svr_dict[key]\n",
        "  print(svr)\n",
        "  # Train the model using the training sets\n",
        "  history=svr.fit(X_train, y_train)\n",
        "#   print(svr.summary())\n",
        "  y_pred_svr = svr.predict(X_test)\n",
        "  model.append(key)\n",
        "\n",
        "  MSE_svr[key] = metrics.mean_squared_error(y_test, y_pred_svr)\n",
        "  MAE_svr[key]=metrics.mean_absolute_error(y_test, y_pred_svr)\n",
        "  RMSE[key]=np.sqrt(metrics.mean_squared_error(y_test, y_pred_svr))\n",
        "  print(\"hello2\")\n",
        "  Var_svr[key] = svr.score(X_test, y_test)\n",
        "  accuracy.append(Var_svr[key])\n",
        "  error.append(MSE_svr[key])\n",
        "  print(\"hello3\")\n",
        "  df = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred_svr.flatten()})\n",
        "  df1 = df\n",
        "  df1.plot(kind='bar', figsize=(16, 10))\n",
        "  plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\n",
        "  plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n",
        "  plt.ylabel('Error Difference')\n",
        "  plt.xlabel('Number of Test Samples')\n",
        "  plt.title('{}'.format(key))\n",
        "  # plt.scatter(X_test, y_test, color='gray')\n",
        "  # plt.plot(X_test, y_pred_svr, color='red', linewidth=2)\n",
        "  plt.savefig(key+'.png', dpi=300, bbox_inches='tight')\n",
        "  plt.show()\n",
        "  \n",
        "#   pyplot.title('Loss / Mean Squared Error')\n",
        "#   pyplot.plot(history.history['loss'], label='train')\n",
        "#   pyplot.plot(history.history['val_loss'], label='test')\n",
        "#   pyplot.legend()\n",
        "#   pyplot.show()\n",
        "  print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_svr))\n",
        "  print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_svr))\n",
        "  print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_svr)))\n",
        "  print(\"Comparision of Models ....\\n\")\n",
        "\n",
        "print(\"\\n Model Name \\t\\t\\t MSE_MLP \\n\")\n",
        "for key in svr_dict.keys():\n",
        "  print(\"{0:20}\\t\\t {1:.5f}\\t\\t{2:.5f}\\n\".format(key, MSE_svr[key],Var_svr[key]))\n",
        "\n",
        "social media and machine learning conferences 2020"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Poly\n",
            "SVR(C=100, cache_size=200, coef0=1, degree=3, epsilon=0.1, gamma='auto',\n",
            "    kernel='poly', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
            "hello2\n",
            "hello3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6oAAAJmCAYAAABVIX1lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1152x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean Absolute Error: 0.06046278222992425\n",
            "Mean Squared Error: 0.008132241661342647\n",
            "Root Mean Squared Error: 0.09017894244967972\n",
            "Comparision of Models ....\n",
            "\n",
            "Linear\n",
            "SVR(C=100, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
            "    kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
            "hello2\n",
            "hello3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6oAAAJmCAYAAABVIX1lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1152x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean Absolute Error: 0.07039604026097754\n",
            "Mean Squared Error: 0.005681885156177823\n",
            "Root Mean Squared Error: 0.07537828040077475\n",
            "Comparision of Models ....\n",
            "\n",
            "RBF\n",
            "SVR(C=100, cache_size=200, coef0=1, degree=3, epsilon=0.1, gamma='auto',\n",
            "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
            "hello2\n",
            "hello3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6oAAAJmCAYAAABVIX1lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVM3MzMzMzGyg/BewdKfwrl4TEAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1152x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean Absolute Error: 0.09657573828819886\n",
            "Mean Squared Error: 0.11970056277007934\n",
            "Root Mean Squared Error: 0.34597769114507854\n",
            "Comparision of Models ....\n",
            "\n",
            "\n",
            " Model Name \t\t\t MSE_MLP \n",
            "\n",
            "Poly                \t\t 0.00813\t\t0.97836\n",
            "\n",
            "Linear              \t\t 0.00568\t\t0.98488\n",
            "\n",
            "RBF                 \t\t 0.11970\t\t0.68146\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4EGbd2FDLeA",
        "colab_type": "code",
        "outputId": "2c0fb815-5f1c-4a8e-ce53-a458b6978420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataframealluser1.csv      Weights-005--0.04119.hdf5  Weights-034--0.01473.hdf5\n",
            "Linear.png                 Weights-009--0.03882.hdf5  Weights-069--0.01456.hdf5\n",
            "Poly.png                   Weights-010--0.02936.hdf5  Weights-072--0.01444.hdf5\n",
            "RBF.png                    Weights-014--0.02933.hdf5  Weights-079--0.01152.hdf5\n",
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/               Weights-016--0.02436.hdf5  Weights-114--0.01140.hdf5\n",
            "Weights-001--0.22423.hdf5  Weights-019--0.02337.hdf5  Weights-170--0.01139.hdf5\n",
            "Weights-002--0.17912.hdf5  Weights-030--0.01963.hdf5  Weights-217--0.00865.hdf5\n",
            "Weights-003--0.07496.hdf5  Weights-031--0.01781.hdf5  Weights-577--0.00844.hdf5\n",
            "Weights-004--0.06436.hdf5  Weights-032--0.01492.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ydya-SLq8_KZ",
        "colab_type": "code",
        "outputId": "f2e752e4-7cb2-456a-ff7a-1b05efbc0755",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# ######################Deep Neural network for regressor################\n",
        "# # Define a sequential model\n",
        "# # Add some dense layers\n",
        "# # Use relu as the activation function for the hidden layers\n",
        "# # Use a normal initializer as the kernal_intializer\n",
        "# # We will use mean_absolute_error as a loss function\n",
        "# # Define the output layer with only one node\n",
        "# # Use linear as the activation function for the output layer\n",
        "#\n",
        "NN_model = Sequential()\n",
        "\n",
        "# The Input Layer :\n",
        "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
        "\n",
        "# The Hidden Layers :\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "# The Output Layer :\n",
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
        "\n",
        "# Compile the network :\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()\n",
        "\n",
        "\n",
        "########Define a check point####################\n",
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5'\n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "history=NN_model.fit(X_train, y_train, epochs=1000, batch_size=32, validation_data=(X_test,y_test), callbacks=callbacks_list)\n",
        "print(len(NN_model.predict(X_test)))\n",
        "# print(NN_model.history.keys())\n",
        "# \"Loss\"\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Mean Squared Error')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.savefig('MSEDNN.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "# plt.savefig('mse.png')\n",
        "\n",
        "\n",
        "# ##########MLP regressor#########################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############################################################################################"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_11 (Dense)             (None, 128)               2048      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 166,913\n",
            "Trainable params: 166,913\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 388 samples, validate on 112 samples\n",
            "Epoch 1/1000\n",
            "388/388 [==============================] - 0s 1ms/step - loss: 0.2935 - mean_absolute_error: 0.2935 - val_loss: 0.2319 - val_mean_absolute_error: 0.2319\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.23188, saving model to Weights-001--0.23188.hdf5\n",
            "Epoch 2/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.1598 - mean_absolute_error: 0.1598 - val_loss: 0.0933 - val_mean_absolute_error: 0.0933\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.23188 to 0.09334, saving model to Weights-002--0.09334.hdf5\n",
            "Epoch 3/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0853 - mean_absolute_error: 0.0853 - val_loss: 0.0708 - val_mean_absolute_error: 0.0708\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.09334 to 0.07084, saving model to Weights-003--0.07084.hdf5\n",
            "Epoch 4/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0620 - mean_absolute_error: 0.0620 - val_loss: 0.0770 - val_mean_absolute_error: 0.0770\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.07084\n",
            "Epoch 5/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0603 - mean_absolute_error: 0.0603 - val_loss: 0.0495 - val_mean_absolute_error: 0.0495\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.07084 to 0.04949, saving model to Weights-005--0.04949.hdf5\n",
            "Epoch 6/1000\n",
            "388/388 [==============================] - 0s 125us/step - loss: 0.0452 - mean_absolute_error: 0.0452 - val_loss: 0.0540 - val_mean_absolute_error: 0.0540\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.04949\n",
            "Epoch 7/1000\n",
            "388/388 [==============================] - 0s 152us/step - loss: 0.0529 - mean_absolute_error: 0.0529 - val_loss: 0.0660 - val_mean_absolute_error: 0.0660\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.04949\n",
            "Epoch 8/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0663 - mean_absolute_error: 0.0663 - val_loss: 0.0354 - val_mean_absolute_error: 0.0354\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.04949 to 0.03540, saving model to Weights-008--0.03540.hdf5\n",
            "Epoch 9/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0320 - mean_absolute_error: 0.0320 - val_loss: 0.0277 - val_mean_absolute_error: 0.0277\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.03540 to 0.02765, saving model to Weights-009--0.02765.hdf5\n",
            "Epoch 10/1000\n",
            "388/388 [==============================] - 0s 153us/step - loss: 0.0235 - mean_absolute_error: 0.0235 - val_loss: 0.0299 - val_mean_absolute_error: 0.0299\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.02765\n",
            "Epoch 11/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0241 - mean_absolute_error: 0.0241 - val_loss: 0.0298 - val_mean_absolute_error: 0.0298\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.02765\n",
            "Epoch 12/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0278 - mean_absolute_error: 0.0278 - val_loss: 0.0285 - val_mean_absolute_error: 0.0285\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.02765\n",
            "Epoch 13/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0267 - mean_absolute_error: 0.0267 - val_loss: 0.0240 - val_mean_absolute_error: 0.0240\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.02765 to 0.02403, saving model to Weights-013--0.02403.hdf5\n",
            "Epoch 14/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0262 - mean_absolute_error: 0.0262 - val_loss: 0.0218 - val_mean_absolute_error: 0.0218\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.02403 to 0.02177, saving model to Weights-014--0.02177.hdf5\n",
            "Epoch 15/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0256 - mean_absolute_error: 0.0256 - val_loss: 0.0631 - val_mean_absolute_error: 0.0631\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.02177\n",
            "Epoch 16/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0417 - mean_absolute_error: 0.0417 - val_loss: 0.0368 - val_mean_absolute_error: 0.0368\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.02177\n",
            "Epoch 17/1000\n",
            "388/388 [==============================] - 0s 145us/step - loss: 0.0310 - mean_absolute_error: 0.0310 - val_loss: 0.0318 - val_mean_absolute_error: 0.0318\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.02177\n",
            "Epoch 18/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0307 - mean_absolute_error: 0.0307 - val_loss: 0.0189 - val_mean_absolute_error: 0.0189\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.02177 to 0.01888, saving model to Weights-018--0.01888.hdf5\n",
            "Epoch 19/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0234 - mean_absolute_error: 0.0234 - val_loss: 0.0195 - val_mean_absolute_error: 0.0195\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01888\n",
            "Epoch 20/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0168 - mean_absolute_error: 0.0168 - val_loss: 0.0149 - val_mean_absolute_error: 0.0149\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.01888 to 0.01492, saving model to Weights-020--0.01492.hdf5\n",
            "Epoch 21/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0143 - mean_absolute_error: 0.0143 - val_loss: 0.0226 - val_mean_absolute_error: 0.0226\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.01492\n",
            "Epoch 22/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0186 - mean_absolute_error: 0.0186 - val_loss: 0.0242 - val_mean_absolute_error: 0.0242\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.01492\n",
            "Epoch 23/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0256 - mean_absolute_error: 0.0256 - val_loss: 0.0293 - val_mean_absolute_error: 0.0293\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.01492\n",
            "Epoch 24/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0223 - mean_absolute_error: 0.0223 - val_loss: 0.0217 - val_mean_absolute_error: 0.0217\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.01492\n",
            "Epoch 25/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0155 - mean_absolute_error: 0.0155 - val_loss: 0.0278 - val_mean_absolute_error: 0.0278\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.01492\n",
            "Epoch 26/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0270 - mean_absolute_error: 0.0270 - val_loss: 0.0381 - val_mean_absolute_error: 0.0381\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.01492\n",
            "Epoch 27/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0215 - mean_absolute_error: 0.0215 - val_loss: 0.0171 - val_mean_absolute_error: 0.0171\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.01492\n",
            "Epoch 28/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0176 - mean_absolute_error: 0.0176 - val_loss: 0.0170 - val_mean_absolute_error: 0.0170\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.01492\n",
            "Epoch 29/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0163 - mean_absolute_error: 0.0163 - val_loss: 0.0344 - val_mean_absolute_error: 0.0344\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.01492\n",
            "Epoch 30/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0227 - mean_absolute_error: 0.0227 - val_loss: 0.0243 - val_mean_absolute_error: 0.0243\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.01492\n",
            "Epoch 31/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0143 - mean_absolute_error: 0.0143 - val_loss: 0.0239 - val_mean_absolute_error: 0.0239\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.01492\n",
            "Epoch 32/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0275 - mean_absolute_error: 0.0275 - val_loss: 0.0477 - val_mean_absolute_error: 0.0477\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.01492\n",
            "Epoch 33/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0244 - mean_absolute_error: 0.0244 - val_loss: 0.0185 - val_mean_absolute_error: 0.0185\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.01492\n",
            "Epoch 34/1000\n",
            "388/388 [==============================] - 0s 151us/step - loss: 0.0127 - mean_absolute_error: 0.0127 - val_loss: 0.0147 - val_mean_absolute_error: 0.0147\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.01492 to 0.01473, saving model to Weights-034--0.01473.hdf5\n",
            "Epoch 35/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0123 - mean_absolute_error: 0.0123 - val_loss: 0.0188 - val_mean_absolute_error: 0.0188\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.01473\n",
            "Epoch 36/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0146 - val_mean_absolute_error: 0.0146\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.01473 to 0.01463, saving model to Weights-036--0.01463.hdf5\n",
            "Epoch 37/1000\n",
            "388/388 [==============================] - 0s 123us/step - loss: 0.0144 - mean_absolute_error: 0.0144 - val_loss: 0.0168 - val_mean_absolute_error: 0.0168\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.01463\n",
            "Epoch 38/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0170 - mean_absolute_error: 0.0170 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.01463 to 0.01255, saving model to Weights-038--0.01255.hdf5\n",
            "Epoch 39/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0166 - mean_absolute_error: 0.0166 - val_loss: 0.0228 - val_mean_absolute_error: 0.0228\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.01255\n",
            "Epoch 40/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0177 - mean_absolute_error: 0.0177 - val_loss: 0.0246 - val_mean_absolute_error: 0.0246\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.01255\n",
            "Epoch 41/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0178 - mean_absolute_error: 0.0178 - val_loss: 0.0175 - val_mean_absolute_error: 0.0175\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.01255\n",
            "Epoch 42/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0162 - mean_absolute_error: 0.0162 - val_loss: 0.0200 - val_mean_absolute_error: 0.0200\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.01255\n",
            "Epoch 43/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0124 - mean_absolute_error: 0.0124 - val_loss: 0.0196 - val_mean_absolute_error: 0.0196\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.01255\n",
            "Epoch 44/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0180 - mean_absolute_error: 0.0180 - val_loss: 0.0145 - val_mean_absolute_error: 0.0145\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.01255\n",
            "Epoch 45/1000\n",
            "388/388 [==============================] - 0s 145us/step - loss: 0.0228 - mean_absolute_error: 0.0228 - val_loss: 0.0293 - val_mean_absolute_error: 0.0293\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.01255\n",
            "Epoch 46/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0212 - mean_absolute_error: 0.0212 - val_loss: 0.0212 - val_mean_absolute_error: 0.0212\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.01255\n",
            "Epoch 47/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0159 - mean_absolute_error: 0.0159 - val_loss: 0.0188 - val_mean_absolute_error: 0.0188\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.01255\n",
            "Epoch 48/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0137 - mean_absolute_error: 0.0137 - val_loss: 0.0206 - val_mean_absolute_error: 0.0206\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.01255\n",
            "Epoch 49/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0126 - mean_absolute_error: 0.0126 - val_loss: 0.0107 - val_mean_absolute_error: 0.0107\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.01255 to 0.01071, saving model to Weights-049--0.01071.hdf5\n",
            "Epoch 50/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0114 - mean_absolute_error: 0.0114 - val_loss: 0.0139 - val_mean_absolute_error: 0.0139\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.01071\n",
            "Epoch 51/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0126 - mean_absolute_error: 0.0126 - val_loss: 0.0312 - val_mean_absolute_error: 0.0312\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.01071\n",
            "Epoch 52/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0432 - mean_absolute_error: 0.0432 - val_loss: 0.0490 - val_mean_absolute_error: 0.0490\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.01071\n",
            "Epoch 53/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0471 - mean_absolute_error: 0.0471 - val_loss: 0.0310 - val_mean_absolute_error: 0.0310\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.01071\n",
            "Epoch 54/1000\n",
            "388/388 [==============================] - 0s 159us/step - loss: 0.0218 - mean_absolute_error: 0.0218 - val_loss: 0.0332 - val_mean_absolute_error: 0.0332\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.01071\n",
            "Epoch 55/1000\n",
            "388/388 [==============================] - 0s 151us/step - loss: 0.0259 - mean_absolute_error: 0.0259 - val_loss: 0.0341 - val_mean_absolute_error: 0.0341\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.01071\n",
            "Epoch 56/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0219 - mean_absolute_error: 0.0219 - val_loss: 0.0191 - val_mean_absolute_error: 0.0191\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.01071\n",
            "Epoch 57/1000\n",
            "388/388 [==============================] - 0s 148us/step - loss: 0.0130 - mean_absolute_error: 0.0130 - val_loss: 0.0264 - val_mean_absolute_error: 0.0264\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.01071\n",
            "Epoch 58/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0143 - mean_absolute_error: 0.0143 - val_loss: 0.0138 - val_mean_absolute_error: 0.0138\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.01071\n",
            "Epoch 59/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0151 - mean_absolute_error: 0.0151 - val_loss: 0.0143 - val_mean_absolute_error: 0.0143\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.01071\n",
            "Epoch 60/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0112 - mean_absolute_error: 0.0112 - val_loss: 0.0258 - val_mean_absolute_error: 0.0258\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.01071\n",
            "Epoch 61/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0204 - mean_absolute_error: 0.0204 - val_loss: 0.0350 - val_mean_absolute_error: 0.0350\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.01071\n",
            "Epoch 62/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0184 - mean_absolute_error: 0.0184 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.01071\n",
            "Epoch 63/1000\n",
            "388/388 [==============================] - 0s 159us/step - loss: 0.0104 - mean_absolute_error: 0.0104 - val_loss: 0.0213 - val_mean_absolute_error: 0.0213\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.01071\n",
            "Epoch 64/1000\n",
            "388/388 [==============================] - 0s 145us/step - loss: 0.0151 - mean_absolute_error: 0.0151 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.01071\n",
            "Epoch 65/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0134 - mean_absolute_error: 0.0134 - val_loss: 0.0114 - val_mean_absolute_error: 0.0114\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.01071\n",
            "Epoch 66/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0161 - mean_absolute_error: 0.0161 - val_loss: 0.0198 - val_mean_absolute_error: 0.0198\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.01071\n",
            "Epoch 67/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0179 - mean_absolute_error: 0.0179 - val_loss: 0.0329 - val_mean_absolute_error: 0.0329\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.01071\n",
            "Epoch 68/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0206 - mean_absolute_error: 0.0206 - val_loss: 0.0138 - val_mean_absolute_error: 0.0138\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.01071\n",
            "Epoch 69/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0115 - mean_absolute_error: 0.0115 - val_loss: 0.0138 - val_mean_absolute_error: 0.0138\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.01071\n",
            "Epoch 70/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0121 - mean_absolute_error: 0.0121 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.01071 to 0.00871, saving model to Weights-070--0.00871.hdf5\n",
            "Epoch 71/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0110 - mean_absolute_error: 0.0110 - val_loss: 0.0173 - val_mean_absolute_error: 0.0173\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.00871\n",
            "Epoch 72/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0124 - mean_absolute_error: 0.0124 - val_loss: 0.0109 - val_mean_absolute_error: 0.0109\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.00871\n",
            "Epoch 73/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0136 - mean_absolute_error: 0.0136 - val_loss: 0.0180 - val_mean_absolute_error: 0.0180\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.00871\n",
            "Epoch 74/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0116 - mean_absolute_error: 0.0116 - val_loss: 0.0250 - val_mean_absolute_error: 0.0250\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.00871\n",
            "Epoch 75/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0194 - mean_absolute_error: 0.0194 - val_loss: 0.0228 - val_mean_absolute_error: 0.0228\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.00871\n",
            "Epoch 76/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0154 - mean_absolute_error: 0.0154 - val_loss: 0.0169 - val_mean_absolute_error: 0.0169\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.00871\n",
            "Epoch 77/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0107 - mean_absolute_error: 0.0107 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.00871\n",
            "Epoch 78/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0100 - mean_absolute_error: 0.0100 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.00871\n",
            "Epoch 79/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0118 - mean_absolute_error: 0.0118 - val_loss: 0.0177 - val_mean_absolute_error: 0.0177\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.00871\n",
            "Epoch 80/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0079 - mean_absolute_error: 0.0079 - val_loss: 0.0159 - val_mean_absolute_error: 0.0159\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.00871\n",
            "Epoch 81/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0072 - mean_absolute_error: 0.0072 - val_loss: 0.0086 - val_mean_absolute_error: 0.0086\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.00871 to 0.00859, saving model to Weights-081--0.00859.hdf5\n",
            "Epoch 82/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0060 - mean_absolute_error: 0.0060 - val_loss: 0.0581 - val_mean_absolute_error: 0.0581\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.00859\n",
            "Epoch 83/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0861 - mean_absolute_error: 0.0861 - val_loss: 0.0448 - val_mean_absolute_error: 0.0448\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.00859\n",
            "Epoch 84/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0411 - mean_absolute_error: 0.0411 - val_loss: 0.0225 - val_mean_absolute_error: 0.0225\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.00859\n",
            "Epoch 85/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0153 - mean_absolute_error: 0.0153 - val_loss: 0.0266 - val_mean_absolute_error: 0.0266\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.00859\n",
            "Epoch 86/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0145 - mean_absolute_error: 0.0145 - val_loss: 0.0109 - val_mean_absolute_error: 0.0109\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.00859\n",
            "Epoch 87/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0096 - mean_absolute_error: 0.0096 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.00859\n",
            "Epoch 88/1000\n",
            "388/388 [==============================] - 0s 146us/step - loss: 0.0142 - mean_absolute_error: 0.0142 - val_loss: 0.0184 - val_mean_absolute_error: 0.0184\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.00859\n",
            "Epoch 89/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0129 - mean_absolute_error: 0.0129 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.00859\n",
            "Epoch 90/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0097 - mean_absolute_error: 0.0097 - val_loss: 0.0162 - val_mean_absolute_error: 0.0162\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.00859\n",
            "Epoch 91/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0137 - mean_absolute_error: 0.0137 - val_loss: 0.0179 - val_mean_absolute_error: 0.0179\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.00859\n",
            "Epoch 92/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0127 - mean_absolute_error: 0.0127 - val_loss: 0.0142 - val_mean_absolute_error: 0.0142\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.00859\n",
            "Epoch 93/1000\n",
            "388/388 [==============================] - 0s 154us/step - loss: 0.0097 - mean_absolute_error: 0.0097 - val_loss: 0.0158 - val_mean_absolute_error: 0.0158\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.00859\n",
            "Epoch 94/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0109 - mean_absolute_error: 0.0109 - val_loss: 0.0193 - val_mean_absolute_error: 0.0193\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.00859\n",
            "Epoch 95/1000\n",
            "388/388 [==============================] - 0s 167us/step - loss: 0.0199 - mean_absolute_error: 0.0199 - val_loss: 0.0164 - val_mean_absolute_error: 0.0164\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.00859\n",
            "Epoch 96/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0140 - mean_absolute_error: 0.0140 - val_loss: 0.0182 - val_mean_absolute_error: 0.0182\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.00859\n",
            "Epoch 97/1000\n",
            "388/388 [==============================] - 0s 149us/step - loss: 0.0170 - mean_absolute_error: 0.0170 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.00859\n",
            "Epoch 98/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0123 - mean_absolute_error: 0.0123 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.00859\n",
            "Epoch 99/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0169 - val_mean_absolute_error: 0.0169\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.00859\n",
            "Epoch 100/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0146 - mean_absolute_error: 0.0146 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.00859\n",
            "Epoch 101/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0101 - mean_absolute_error: 0.0101 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.00859\n",
            "Epoch 102/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0074 - mean_absolute_error: 0.0074 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.00859\n",
            "Epoch 103/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0085 - mean_absolute_error: 0.0085 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.00859\n",
            "Epoch 104/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0083 - mean_absolute_error: 0.0083 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.00859\n",
            "Epoch 105/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0164 - val_mean_absolute_error: 0.0164\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.00859\n",
            "Epoch 106/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0125 - mean_absolute_error: 0.0125 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.00859\n",
            "Epoch 107/1000\n",
            "388/388 [==============================] - 0s 123us/step - loss: 0.0112 - mean_absolute_error: 0.0112 - val_loss: 0.0143 - val_mean_absolute_error: 0.0143\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.00859\n",
            "Epoch 108/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0107 - mean_absolute_error: 0.0107 - val_loss: 0.0107 - val_mean_absolute_error: 0.0107\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.00859\n",
            "Epoch 109/1000\n",
            "388/388 [==============================] - 0s 149us/step - loss: 0.0072 - mean_absolute_error: 0.0072 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.00859\n",
            "Epoch 110/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0075 - mean_absolute_error: 0.0075 - val_loss: 0.0112 - val_mean_absolute_error: 0.0112\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.00859\n",
            "Epoch 111/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0069 - mean_absolute_error: 0.0069 - val_loss: 0.0148 - val_mean_absolute_error: 0.0148\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.00859\n",
            "Epoch 112/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0085 - mean_absolute_error: 0.0085 - val_loss: 0.0095 - val_mean_absolute_error: 0.0095\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.00859\n",
            "Epoch 113/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0064 - mean_absolute_error: 0.0064 - val_loss: 0.0070 - val_mean_absolute_error: 0.0070\n",
            "\n",
            "Epoch 00113: val_loss improved from 0.00859 to 0.00701, saving model to Weights-113--0.00701.hdf5\n",
            "Epoch 114/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.00701\n",
            "Epoch 115/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0075 - mean_absolute_error: 0.0075 - val_loss: 0.0136 - val_mean_absolute_error: 0.0136\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.00701\n",
            "Epoch 116/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0102 - mean_absolute_error: 0.0102 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.00701\n",
            "Epoch 117/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0110 - mean_absolute_error: 0.0110 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.00701\n",
            "Epoch 118/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0078 - mean_absolute_error: 0.0078 - val_loss: 0.0295 - val_mean_absolute_error: 0.0295\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.00701\n",
            "Epoch 119/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0213 - mean_absolute_error: 0.0213 - val_loss: 0.0267 - val_mean_absolute_error: 0.0267\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.00701\n",
            "Epoch 120/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0179 - mean_absolute_error: 0.0179 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.00701\n",
            "Epoch 121/1000\n",
            "388/388 [==============================] - 0s 148us/step - loss: 0.0079 - mean_absolute_error: 0.0079 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.00701\n",
            "Epoch 122/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0111 - mean_absolute_error: 0.0111 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.00701\n",
            "Epoch 123/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0064 - mean_absolute_error: 0.0064 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.00701\n",
            "Epoch 124/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0065 - mean_absolute_error: 0.0065 - val_loss: 0.0108 - val_mean_absolute_error: 0.0108\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.00701\n",
            "Epoch 125/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0066 - mean_absolute_error: 0.0066 - val_loss: 0.0077 - val_mean_absolute_error: 0.0077\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.00701\n",
            "Epoch 126/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0109 - val_mean_absolute_error: 0.0109\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.00701\n",
            "Epoch 127/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0074 - mean_absolute_error: 0.0074 - val_loss: 0.0078 - val_mean_absolute_error: 0.0078\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.00701\n",
            "Epoch 128/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0087 - mean_absolute_error: 0.0087 - val_loss: 0.0095 - val_mean_absolute_error: 0.0095\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.00701\n",
            "Epoch 129/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.00701\n",
            "Epoch 130/1000\n",
            "388/388 [==============================] - 0s 149us/step - loss: 0.0070 - mean_absolute_error: 0.0070 - val_loss: 0.0095 - val_mean_absolute_error: 0.0095\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.00701\n",
            "Epoch 131/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0087 - mean_absolute_error: 0.0087 - val_loss: 0.0152 - val_mean_absolute_error: 0.0152\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.00701\n",
            "Epoch 132/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0072 - mean_absolute_error: 0.0072 - val_loss: 0.0109 - val_mean_absolute_error: 0.0109\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.00701\n",
            "Epoch 133/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0063 - mean_absolute_error: 0.0063 - val_loss: 0.0072 - val_mean_absolute_error: 0.0072\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.00701\n",
            "Epoch 134/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0062 - mean_absolute_error: 0.0062 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.00701\n",
            "Epoch 135/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0124 - mean_absolute_error: 0.0124 - val_loss: 0.0140 - val_mean_absolute_error: 0.0140\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.00701\n",
            "Epoch 136/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0106 - mean_absolute_error: 0.0106 - val_loss: 0.0160 - val_mean_absolute_error: 0.0160\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.00701\n",
            "Epoch 137/1000\n",
            "388/388 [==============================] - 0s 121us/step - loss: 0.0076 - mean_absolute_error: 0.0076 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.00701\n",
            "Epoch 138/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0085 - mean_absolute_error: 0.0085 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.00701\n",
            "Epoch 139/1000\n",
            "388/388 [==============================] - 0s 119us/step - loss: 0.0103 - mean_absolute_error: 0.0103 - val_loss: 0.0160 - val_mean_absolute_error: 0.0160\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.00701\n",
            "Epoch 140/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0160 - val_mean_absolute_error: 0.0160\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.00701\n",
            "Epoch 141/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0105 - mean_absolute_error: 0.0105 - val_loss: 0.0138 - val_mean_absolute_error: 0.0138\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.00701\n",
            "Epoch 142/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0126 - mean_absolute_error: 0.0126 - val_loss: 0.0146 - val_mean_absolute_error: 0.0146\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.00701\n",
            "Epoch 143/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0122 - mean_absolute_error: 0.0122 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.00701\n",
            "Epoch 144/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0143 - mean_absolute_error: 0.0143 - val_loss: 0.0243 - val_mean_absolute_error: 0.0243\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.00701\n",
            "Epoch 145/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0158 - mean_absolute_error: 0.0158 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.00701\n",
            "Epoch 146/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0103 - mean_absolute_error: 0.0103 - val_loss: 0.0083 - val_mean_absolute_error: 0.0083\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.00701\n",
            "Epoch 147/1000\n",
            "388/388 [==============================] - 0s 152us/step - loss: 0.0082 - mean_absolute_error: 0.0082 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.00701\n",
            "Epoch 148/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0071 - mean_absolute_error: 0.0071 - val_loss: 0.0078 - val_mean_absolute_error: 0.0078\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.00701\n",
            "Epoch 149/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0081 - val_mean_absolute_error: 0.0081\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.00701\n",
            "Epoch 150/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0077 - mean_absolute_error: 0.0077 - val_loss: 0.0114 - val_mean_absolute_error: 0.0114\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.00701\n",
            "Epoch 151/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0151 - val_mean_absolute_error: 0.0151\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.00701\n",
            "Epoch 152/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0131 - mean_absolute_error: 0.0131 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.00701\n",
            "Epoch 153/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0067 - mean_absolute_error: 0.0067 - val_loss: 0.0084 - val_mean_absolute_error: 0.0084\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.00701\n",
            "Epoch 154/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0063 - mean_absolute_error: 0.0063 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.00701\n",
            "Epoch 155/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0079 - mean_absolute_error: 0.0079 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.00701\n",
            "Epoch 156/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0069 - mean_absolute_error: 0.0069 - val_loss: 0.0098 - val_mean_absolute_error: 0.0098\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.00701\n",
            "Epoch 157/1000\n",
            "388/388 [==============================] - 0s 145us/step - loss: 0.0078 - mean_absolute_error: 0.0078 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.00701\n",
            "Epoch 158/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0083 - mean_absolute_error: 0.0083 - val_loss: 0.0201 - val_mean_absolute_error: 0.0201\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.00701\n",
            "Epoch 159/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0131 - mean_absolute_error: 0.0131 - val_loss: 0.0291 - val_mean_absolute_error: 0.0291\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.00701\n",
            "Epoch 160/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0298 - mean_absolute_error: 0.0298 - val_loss: 0.0404 - val_mean_absolute_error: 0.0404\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.00701\n",
            "Epoch 161/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0219 - mean_absolute_error: 0.0219 - val_loss: 0.0255 - val_mean_absolute_error: 0.0255\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.00701\n",
            "Epoch 162/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0147 - mean_absolute_error: 0.0147 - val_loss: 0.0146 - val_mean_absolute_error: 0.0146\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.00701\n",
            "Epoch 163/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0086 - mean_absolute_error: 0.0086 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.00701\n",
            "Epoch 164/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0066 - mean_absolute_error: 0.0066 - val_loss: 0.0140 - val_mean_absolute_error: 0.0140\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.00701\n",
            "Epoch 165/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0086 - mean_absolute_error: 0.0086 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.00701\n",
            "Epoch 166/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0079 - mean_absolute_error: 0.0079 - val_loss: 0.0158 - val_mean_absolute_error: 0.0158\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.00701\n",
            "Epoch 167/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0073 - mean_absolute_error: 0.0073 - val_loss: 0.0078 - val_mean_absolute_error: 0.0078\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.00701\n",
            "Epoch 168/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0076 - mean_absolute_error: 0.0076 - val_loss: 0.0203 - val_mean_absolute_error: 0.0203\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.00701\n",
            "Epoch 169/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0093 - mean_absolute_error: 0.0093 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.00701\n",
            "Epoch 170/1000\n",
            "388/388 [==============================] - 0s 145us/step - loss: 0.0099 - mean_absolute_error: 0.0099 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.00701\n",
            "Epoch 171/1000\n",
            "388/388 [==============================] - 0s 153us/step - loss: 0.0077 - mean_absolute_error: 0.0077 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.00701\n",
            "Epoch 172/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0122 - mean_absolute_error: 0.0122 - val_loss: 0.0223 - val_mean_absolute_error: 0.0223\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.00701\n",
            "Epoch 173/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.0228 - val_mean_absolute_error: 0.0228\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.00701\n",
            "Epoch 174/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0147 - mean_absolute_error: 0.0147 - val_loss: 0.0164 - val_mean_absolute_error: 0.0164\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.00701\n",
            "Epoch 175/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.00701\n",
            "Epoch 176/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0059 - mean_absolute_error: 0.0059 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069\n",
            "\n",
            "Epoch 00176: val_loss improved from 0.00701 to 0.00691, saving model to Weights-176--0.00691.hdf5\n",
            "Epoch 177/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.00691\n",
            "Epoch 178/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0064 - mean_absolute_error: 0.0064 - val_loss: 0.0075 - val_mean_absolute_error: 0.0075\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.00691\n",
            "Epoch 179/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0065 - mean_absolute_error: 0.0065 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.00691\n",
            "Epoch 180/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0115 - mean_absolute_error: 0.0115 - val_loss: 0.0080 - val_mean_absolute_error: 0.0080\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.00691\n",
            "Epoch 181/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0068 - mean_absolute_error: 0.0068 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.00691\n",
            "Epoch 182/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0093 - mean_absolute_error: 0.0093 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.00691\n",
            "Epoch 183/1000\n",
            "388/388 [==============================] - 0s 152us/step - loss: 0.0074 - mean_absolute_error: 0.0074 - val_loss: 0.0145 - val_mean_absolute_error: 0.0145\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.00691\n",
            "Epoch 184/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0074 - val_mean_absolute_error: 0.0074\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.00691\n",
            "Epoch 185/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0077 - mean_absolute_error: 0.0077 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.00691\n",
            "Epoch 186/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0121 - mean_absolute_error: 0.0121 - val_loss: 0.0110 - val_mean_absolute_error: 0.0110\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.00691\n",
            "Epoch 187/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0134 - mean_absolute_error: 0.0134 - val_loss: 0.0162 - val_mean_absolute_error: 0.0162\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.00691\n",
            "Epoch 188/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0115 - mean_absolute_error: 0.0115 - val_loss: 0.0181 - val_mean_absolute_error: 0.0181\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.00691\n",
            "Epoch 189/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0115 - mean_absolute_error: 0.0115 - val_loss: 0.0179 - val_mean_absolute_error: 0.0179\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.00691\n",
            "Epoch 190/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0111 - mean_absolute_error: 0.0111 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.00691\n",
            "Epoch 191/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0110 - mean_absolute_error: 0.0110 - val_loss: 0.0221 - val_mean_absolute_error: 0.0221\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.00691\n",
            "Epoch 192/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0135 - mean_absolute_error: 0.0135 - val_loss: 0.0275 - val_mean_absolute_error: 0.0275\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.00691\n",
            "Epoch 193/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0139 - mean_absolute_error: 0.0139 - val_loss: 0.0135 - val_mean_absolute_error: 0.0135\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.00691\n",
            "Epoch 194/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0139 - mean_absolute_error: 0.0139 - val_loss: 0.0078 - val_mean_absolute_error: 0.0078\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.00691\n",
            "Epoch 195/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0105 - mean_absolute_error: 0.0105 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.00691\n",
            "Epoch 196/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0096 - mean_absolute_error: 0.0096 - val_loss: 0.0197 - val_mean_absolute_error: 0.0197\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.00691\n",
            "Epoch 197/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0105 - mean_absolute_error: 0.0105 - val_loss: 0.0148 - val_mean_absolute_error: 0.0148\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.00691\n",
            "Epoch 198/1000\n",
            "388/388 [==============================] - 0s 125us/step - loss: 0.0093 - mean_absolute_error: 0.0093 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.00691\n",
            "Epoch 199/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0077 - mean_absolute_error: 0.0077 - val_loss: 0.0083 - val_mean_absolute_error: 0.0083\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.00691\n",
            "Epoch 200/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0073 - mean_absolute_error: 0.0073 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.00691\n",
            "Epoch 201/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0136 - mean_absolute_error: 0.0136 - val_loss: 0.0219 - val_mean_absolute_error: 0.0219\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.00691\n",
            "Epoch 202/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0133 - mean_absolute_error: 0.0133 - val_loss: 0.0154 - val_mean_absolute_error: 0.0154\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.00691\n",
            "Epoch 203/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.00691\n",
            "Epoch 204/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0135 - val_mean_absolute_error: 0.0135\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.00691\n",
            "Epoch 205/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0076 - mean_absolute_error: 0.0076 - val_loss: 0.0086 - val_mean_absolute_error: 0.0086\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.00691\n",
            "Epoch 206/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0054 - mean_absolute_error: 0.0054 - val_loss: 0.0101 - val_mean_absolute_error: 0.0101\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.00691\n",
            "Epoch 207/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0049 - mean_absolute_error: 0.0049 - val_loss: 0.0110 - val_mean_absolute_error: 0.0110\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.00691\n",
            "Epoch 208/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0057 - mean_absolute_error: 0.0057 - val_loss: 0.0149 - val_mean_absolute_error: 0.0149\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.00691\n",
            "Epoch 209/1000\n",
            "388/388 [==============================] - 0s 146us/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.00691\n",
            "Epoch 210/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0109 - mean_absolute_error: 0.0109 - val_loss: 0.0151 - val_mean_absolute_error: 0.0151\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.00691\n",
            "Epoch 211/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0159 - mean_absolute_error: 0.0159 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.00691\n",
            "Epoch 212/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0063 - mean_absolute_error: 0.0063 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.00691\n",
            "Epoch 213/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0069 - mean_absolute_error: 0.0069 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.00691\n",
            "Epoch 214/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0082 - mean_absolute_error: 0.0082 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.00691\n",
            "Epoch 215/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0115 - mean_absolute_error: 0.0115 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.00691\n",
            "Epoch 216/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0083 - mean_absolute_error: 0.0083 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.00691\n",
            "Epoch 217/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.0168 - val_mean_absolute_error: 0.0168\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.00691\n",
            "Epoch 218/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0080 - mean_absolute_error: 0.0080 - val_loss: 0.0081 - val_mean_absolute_error: 0.0081\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.00691\n",
            "Epoch 219/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0065 - mean_absolute_error: 0.0065 - val_loss: 0.0200 - val_mean_absolute_error: 0.0200\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.00691\n",
            "Epoch 220/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0154 - mean_absolute_error: 0.0154 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.00691\n",
            "Epoch 221/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0149 - mean_absolute_error: 0.0149 - val_loss: 0.0141 - val_mean_absolute_error: 0.0141\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.00691\n",
            "Epoch 222/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0116 - mean_absolute_error: 0.0116 - val_loss: 0.0101 - val_mean_absolute_error: 0.0101\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.00691\n",
            "Epoch 223/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0058 - mean_absolute_error: 0.0058 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.00691\n",
            "Epoch 224/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0057 - mean_absolute_error: 0.0057 - val_loss: 0.0189 - val_mean_absolute_error: 0.0189\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.00691\n",
            "Epoch 225/1000\n",
            "388/388 [==============================] - 0s 150us/step - loss: 0.0165 - mean_absolute_error: 0.0165 - val_loss: 0.0202 - val_mean_absolute_error: 0.0202\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.00691\n",
            "Epoch 226/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0138 - mean_absolute_error: 0.0138 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.00691\n",
            "Epoch 227/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0055 - mean_absolute_error: 0.0055 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.00691\n",
            "Epoch 228/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.00691\n",
            "Epoch 229/1000\n",
            "388/388 [==============================] - 0s 145us/step - loss: 0.0072 - mean_absolute_error: 0.0072 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.00691\n",
            "Epoch 230/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0075 - mean_absolute_error: 0.0075 - val_loss: 0.0312 - val_mean_absolute_error: 0.0312\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.00691\n",
            "Epoch 231/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0299 - val_mean_absolute_error: 0.0299\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.00691\n",
            "Epoch 232/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0250 - mean_absolute_error: 0.0250 - val_loss: 0.0182 - val_mean_absolute_error: 0.0182\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.00691\n",
            "Epoch 233/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0155 - mean_absolute_error: 0.0155 - val_loss: 0.0157 - val_mean_absolute_error: 0.0157\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.00691\n",
            "Epoch 234/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0114 - mean_absolute_error: 0.0114 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.00691\n",
            "Epoch 235/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0097 - mean_absolute_error: 0.0097 - val_loss: 0.0145 - val_mean_absolute_error: 0.0145\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.00691\n",
            "Epoch 236/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0115 - mean_absolute_error: 0.0115 - val_loss: 0.0164 - val_mean_absolute_error: 0.0164\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.00691\n",
            "Epoch 237/1000\n",
            "388/388 [==============================] - 0s 149us/step - loss: 0.0095 - mean_absolute_error: 0.0095 - val_loss: 0.0229 - val_mean_absolute_error: 0.0229\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.00691\n",
            "Epoch 238/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0110 - mean_absolute_error: 0.0110 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.00691\n",
            "Epoch 239/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0087 - mean_absolute_error: 0.0087 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.00691\n",
            "Epoch 240/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0105 - mean_absolute_error: 0.0105 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.00691\n",
            "Epoch 241/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0105 - mean_absolute_error: 0.0105 - val_loss: 0.0136 - val_mean_absolute_error: 0.0136\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.00691\n",
            "Epoch 242/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0078 - mean_absolute_error: 0.0078 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.00691\n",
            "Epoch 243/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0071 - mean_absolute_error: 0.0071 - val_loss: 0.0151 - val_mean_absolute_error: 0.0151\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.00691\n",
            "Epoch 244/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0075 - mean_absolute_error: 0.0075 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.00691\n",
            "Epoch 245/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0096 - mean_absolute_error: 0.0096 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.00691\n",
            "Epoch 246/1000\n",
            "388/388 [==============================] - 0s 154us/step - loss: 0.0064 - mean_absolute_error: 0.0064 - val_loss: 0.0071 - val_mean_absolute_error: 0.0071\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.00691\n",
            "Epoch 247/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.00691\n",
            "Epoch 248/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0066 - mean_absolute_error: 0.0066 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.00691\n",
            "Epoch 249/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0065 - mean_absolute_error: 0.0065 - val_loss: 0.0176 - val_mean_absolute_error: 0.0176\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.00691\n",
            "Epoch 250/1000\n",
            "388/388 [==============================] - 0s 148us/step - loss: 0.0099 - mean_absolute_error: 0.0099 - val_loss: 0.0283 - val_mean_absolute_error: 0.0283\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.00691\n",
            "Epoch 251/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0131 - mean_absolute_error: 0.0131 - val_loss: 0.0076 - val_mean_absolute_error: 0.0076\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.00691\n",
            "Epoch 252/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0072 - mean_absolute_error: 0.0072 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.00691\n",
            "Epoch 253/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0066 - mean_absolute_error: 0.0066 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.00691\n",
            "Epoch 254/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0057 - mean_absolute_error: 0.0057 - val_loss: 0.0105 - val_mean_absolute_error: 0.0105\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.00691\n",
            "Epoch 255/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0069 - mean_absolute_error: 0.0069 - val_loss: 0.0078 - val_mean_absolute_error: 0.0078\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.00691\n",
            "Epoch 256/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0080 - val_mean_absolute_error: 0.0080\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.00691\n",
            "Epoch 257/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0059 - mean_absolute_error: 0.0059 - val_loss: 0.0105 - val_mean_absolute_error: 0.0105\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.00691\n",
            "Epoch 258/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0074 - mean_absolute_error: 0.0074 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.00691\n",
            "Epoch 259/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0077 - mean_absolute_error: 0.0077 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.00691\n",
            "Epoch 260/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0097 - mean_absolute_error: 0.0097 - val_loss: 0.0213 - val_mean_absolute_error: 0.0213\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.00691\n",
            "Epoch 261/1000\n",
            "388/388 [==============================] - 0s 150us/step - loss: 0.0106 - mean_absolute_error: 0.0106 - val_loss: 0.0144 - val_mean_absolute_error: 0.0144\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.00691\n",
            "Epoch 262/1000\n",
            "388/388 [==============================] - 0s 146us/step - loss: 0.0055 - mean_absolute_error: 0.0055 - val_loss: 0.0079 - val_mean_absolute_error: 0.0079\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.00691\n",
            "Epoch 263/1000\n",
            "388/388 [==============================] - 0s 160us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0072 - val_mean_absolute_error: 0.0072\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.00691\n",
            "Epoch 264/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0101 - val_mean_absolute_error: 0.0101\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.00691\n",
            "Epoch 265/1000\n",
            "388/388 [==============================] - 0s 145us/step - loss: 0.0100 - mean_absolute_error: 0.0100 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.00691\n",
            "Epoch 266/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0061 - mean_absolute_error: 0.0061 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.00691\n",
            "Epoch 267/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0062 - mean_absolute_error: 0.0062 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.00691\n",
            "Epoch 268/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0063 - mean_absolute_error: 0.0063 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.00691\n",
            "Epoch 269/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0068 - mean_absolute_error: 0.0068 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.00691\n",
            "Epoch 270/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0080 - mean_absolute_error: 0.0080 - val_loss: 0.0273 - val_mean_absolute_error: 0.0273\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.00691\n",
            "Epoch 271/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0172 - mean_absolute_error: 0.0172 - val_loss: 0.0325 - val_mean_absolute_error: 0.0325\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.00691\n",
            "Epoch 272/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0159 - mean_absolute_error: 0.0159 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.00691\n",
            "Epoch 273/1000\n",
            "388/388 [==============================] - 0s 146us/step - loss: 0.0203 - mean_absolute_error: 0.0203 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.00691\n",
            "Epoch 274/1000\n",
            "388/388 [==============================] - 0s 148us/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.00691\n",
            "Epoch 275/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0074 - mean_absolute_error: 0.0074 - val_loss: 0.0071 - val_mean_absolute_error: 0.0071\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.00691\n",
            "Epoch 276/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0063 - mean_absolute_error: 0.0063 - val_loss: 0.0084 - val_mean_absolute_error: 0.0084\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.00691\n",
            "Epoch 277/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0106 - mean_absolute_error: 0.0106 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.00691\n",
            "Epoch 278/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0126 - mean_absolute_error: 0.0126 - val_loss: 0.0114 - val_mean_absolute_error: 0.0114\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.00691\n",
            "Epoch 279/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0069 - mean_absolute_error: 0.0069 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.00691\n",
            "Epoch 280/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0072 - mean_absolute_error: 0.0072 - val_loss: 0.0168 - val_mean_absolute_error: 0.0168\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.00691\n",
            "Epoch 281/1000\n",
            "388/388 [==============================] - 0s 152us/step - loss: 0.0106 - mean_absolute_error: 0.0106 - val_loss: 0.0251 - val_mean_absolute_error: 0.0251\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.00691\n",
            "Epoch 282/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0164 - mean_absolute_error: 0.0164 - val_loss: 0.0178 - val_mean_absolute_error: 0.0178\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.00691\n",
            "Epoch 283/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0102 - mean_absolute_error: 0.0102 - val_loss: 0.0108 - val_mean_absolute_error: 0.0108\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.00691\n",
            "Epoch 284/1000\n",
            "388/388 [==============================] - 0s 195us/step - loss: 0.0065 - mean_absolute_error: 0.0065 - val_loss: 0.0154 - val_mean_absolute_error: 0.0154\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.00691\n",
            "Epoch 285/1000\n",
            "388/388 [==============================] - 0s 154us/step - loss: 0.0086 - mean_absolute_error: 0.0086 - val_loss: 0.0107 - val_mean_absolute_error: 0.0107\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.00691\n",
            "Epoch 286/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0078 - mean_absolute_error: 0.0078 - val_loss: 0.0179 - val_mean_absolute_error: 0.0179\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.00691\n",
            "Epoch 287/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0115 - mean_absolute_error: 0.0115 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.00691\n",
            "Epoch 288/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0142 - mean_absolute_error: 0.0142 - val_loss: 0.0183 - val_mean_absolute_error: 0.0183\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.00691\n",
            "Epoch 289/1000\n",
            "388/388 [==============================] - 0s 145us/step - loss: 0.0082 - mean_absolute_error: 0.0082 - val_loss: 0.0105 - val_mean_absolute_error: 0.0105\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.00691\n",
            "Epoch 290/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0086 - mean_absolute_error: 0.0086 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.00691\n",
            "Epoch 291/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0104 - mean_absolute_error: 0.0104 - val_loss: 0.0165 - val_mean_absolute_error: 0.0165\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.00691\n",
            "Epoch 292/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0108 - mean_absolute_error: 0.0108 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.00691\n",
            "Epoch 293/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0102 - mean_absolute_error: 0.0102 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.00691\n",
            "Epoch 294/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0062 - mean_absolute_error: 0.0062 - val_loss: 0.0216 - val_mean_absolute_error: 0.0216\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.00691\n",
            "Epoch 295/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0239 - mean_absolute_error: 0.0239 - val_loss: 0.0216 - val_mean_absolute_error: 0.0216\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.00691\n",
            "Epoch 296/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0205 - mean_absolute_error: 0.0205 - val_loss: 0.0162 - val_mean_absolute_error: 0.0162\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.00691\n",
            "Epoch 297/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0107 - mean_absolute_error: 0.0107 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.00691\n",
            "Epoch 298/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0110 - val_mean_absolute_error: 0.0110\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.00691\n",
            "Epoch 299/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0077 - mean_absolute_error: 0.0077 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069\n",
            "\n",
            "Epoch 00299: val_loss improved from 0.00691 to 0.00690, saving model to Weights-299--0.00690.hdf5\n",
            "Epoch 300/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0048 - mean_absolute_error: 0.0048 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.00690\n",
            "Epoch 301/1000\n",
            "388/388 [==============================] - 0s 153us/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.00690\n",
            "Epoch 302/1000\n",
            "388/388 [==============================] - 0s 146us/step - loss: 0.0082 - mean_absolute_error: 0.0082 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.00690\n",
            "Epoch 303/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.00690\n",
            "Epoch 304/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0060 - mean_absolute_error: 0.0060 - val_loss: 0.0080 - val_mean_absolute_error: 0.0080\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.00690\n",
            "Epoch 305/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0068 - mean_absolute_error: 0.0068 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.00690\n",
            "Epoch 306/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0072 - mean_absolute_error: 0.0072 - val_loss: 0.0078 - val_mean_absolute_error: 0.0078\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.00690\n",
            "Epoch 307/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0071 - mean_absolute_error: 0.0071 - val_loss: 0.0174 - val_mean_absolute_error: 0.0174\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.00690\n",
            "Epoch 308/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0114 - mean_absolute_error: 0.0114 - val_loss: 0.0061 - val_mean_absolute_error: 0.0061\n",
            "\n",
            "Epoch 00308: val_loss improved from 0.00690 to 0.00611, saving model to Weights-308--0.00611.hdf5\n",
            "Epoch 309/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0078 - mean_absolute_error: 0.0078 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.00611\n",
            "Epoch 310/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0056 - mean_absolute_error: 0.0056 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.00611\n",
            "Epoch 311/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0053 - mean_absolute_error: 0.0053 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.00611\n",
            "Epoch 312/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0083 - mean_absolute_error: 0.0083 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.00611\n",
            "Epoch 313/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0116 - mean_absolute_error: 0.0116 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.00611\n",
            "Epoch 314/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0063 - mean_absolute_error: 0.0063 - val_loss: 0.0108 - val_mean_absolute_error: 0.0108\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.00611\n",
            "Epoch 315/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0052 - mean_absolute_error: 0.0052 - val_loss: 0.0080 - val_mean_absolute_error: 0.0080\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.00611\n",
            "Epoch 316/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0051 - mean_absolute_error: 0.0051 - val_loss: 0.0083 - val_mean_absolute_error: 0.0083\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.00611\n",
            "Epoch 317/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0075 - mean_absolute_error: 0.0075 - val_loss: 0.0250 - val_mean_absolute_error: 0.0250\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.00611\n",
            "Epoch 318/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0197 - mean_absolute_error: 0.0197 - val_loss: 0.0176 - val_mean_absolute_error: 0.0176\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.00611\n",
            "Epoch 319/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0149 - mean_absolute_error: 0.0149 - val_loss: 0.0105 - val_mean_absolute_error: 0.0105\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.00611\n",
            "Epoch 320/1000\n",
            "388/388 [==============================] - 0s 146us/step - loss: 0.0103 - mean_absolute_error: 0.0103 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.00611\n",
            "Epoch 321/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0080 - mean_absolute_error: 0.0080 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.00611\n",
            "Epoch 322/1000\n",
            "388/388 [==============================] - 0s 153us/step - loss: 0.0060 - mean_absolute_error: 0.0060 - val_loss: 0.0068 - val_mean_absolute_error: 0.0068\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.00611\n",
            "Epoch 323/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0054 - mean_absolute_error: 0.0054 - val_loss: 0.0077 - val_mean_absolute_error: 0.0077\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.00611\n",
            "Epoch 324/1000\n",
            "388/388 [==============================] - 0s 125us/step - loss: 0.0060 - mean_absolute_error: 0.0060 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.00611\n",
            "Epoch 325/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.00611\n",
            "Epoch 326/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0115 - mean_absolute_error: 0.0115 - val_loss: 0.0074 - val_mean_absolute_error: 0.0074\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.00611\n",
            "Epoch 327/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0111 - mean_absolute_error: 0.0111 - val_loss: 0.0098 - val_mean_absolute_error: 0.0098\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.00611\n",
            "Epoch 328/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0074 - mean_absolute_error: 0.0074 - val_loss: 0.0073 - val_mean_absolute_error: 0.0073\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.00611\n",
            "Epoch 329/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.00611\n",
            "Epoch 330/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0072 - mean_absolute_error: 0.0072 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.00611\n",
            "Epoch 331/1000\n",
            "388/388 [==============================] - 0s 146us/step - loss: 0.0093 - mean_absolute_error: 0.0093 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.00611\n",
            "Epoch 332/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0060 - mean_absolute_error: 0.0060 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.00611\n",
            "Epoch 333/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0101 - mean_absolute_error: 0.0101 - val_loss: 0.0074 - val_mean_absolute_error: 0.0074\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.00611\n",
            "Epoch 334/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0075 - mean_absolute_error: 0.0075 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.00611\n",
            "Epoch 335/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0063 - mean_absolute_error: 0.0063 - val_loss: 0.0077 - val_mean_absolute_error: 0.0077\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.00611\n",
            "Epoch 336/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0045 - mean_absolute_error: 0.0045 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.00611\n",
            "Epoch 337/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0147 - val_mean_absolute_error: 0.0147\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.00611\n",
            "Epoch 338/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0093 - mean_absolute_error: 0.0093 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.00611\n",
            "Epoch 339/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0078 - mean_absolute_error: 0.0078 - val_loss: 0.0086 - val_mean_absolute_error: 0.0086\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.00611\n",
            "Epoch 340/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0101 - mean_absolute_error: 0.0101 - val_loss: 0.0190 - val_mean_absolute_error: 0.0190\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.00611\n",
            "Epoch 341/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0171 - val_mean_absolute_error: 0.0171\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.00611\n",
            "Epoch 342/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0085 - mean_absolute_error: 0.0085 - val_loss: 0.0072 - val_mean_absolute_error: 0.0072\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.00611\n",
            "Epoch 343/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0054 - mean_absolute_error: 0.0054 - val_loss: 0.0065 - val_mean_absolute_error: 0.0065\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.00611\n",
            "Epoch 344/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0048 - mean_absolute_error: 0.0048 - val_loss: 0.0073 - val_mean_absolute_error: 0.0073\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.00611\n",
            "Epoch 345/1000\n",
            "388/388 [==============================] - 0s 162us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0079 - val_mean_absolute_error: 0.0079\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.00611\n",
            "Epoch 346/1000\n",
            "388/388 [==============================] - 0s 229us/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0070 - val_mean_absolute_error: 0.0070\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.00611\n",
            "Epoch 347/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0078 - mean_absolute_error: 0.0078 - val_loss: 0.0159 - val_mean_absolute_error: 0.0159\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.00611\n",
            "Epoch 348/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0112 - mean_absolute_error: 0.0112 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.00611\n",
            "Epoch 349/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0101 - mean_absolute_error: 0.0101 - val_loss: 0.0180 - val_mean_absolute_error: 0.0180\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.00611\n",
            "Epoch 350/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0109 - mean_absolute_error: 0.0109 - val_loss: 0.0195 - val_mean_absolute_error: 0.0195\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.00611\n",
            "Epoch 351/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0102 - mean_absolute_error: 0.0102 - val_loss: 0.0081 - val_mean_absolute_error: 0.0081\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.00611\n",
            "Epoch 352/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0065 - mean_absolute_error: 0.0065 - val_loss: 0.0065 - val_mean_absolute_error: 0.0065\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.00611\n",
            "Epoch 353/1000\n",
            "388/388 [==============================] - 0s 163us/step - loss: 0.0053 - mean_absolute_error: 0.0053 - val_loss: 0.0306 - val_mean_absolute_error: 0.0306\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.00611\n",
            "Epoch 354/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0367 - mean_absolute_error: 0.0367 - val_loss: 0.0195 - val_mean_absolute_error: 0.0195\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.00611\n",
            "Epoch 355/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0265 - mean_absolute_error: 0.0265 - val_loss: 0.0266 - val_mean_absolute_error: 0.0266\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.00611\n",
            "Epoch 356/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0221 - mean_absolute_error: 0.0221 - val_loss: 0.0074 - val_mean_absolute_error: 0.0074\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.00611\n",
            "Epoch 357/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0157 - mean_absolute_error: 0.0157 - val_loss: 0.0147 - val_mean_absolute_error: 0.0147\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.00611\n",
            "Epoch 358/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0143 - mean_absolute_error: 0.0143 - val_loss: 0.0172 - val_mean_absolute_error: 0.0172\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.00611\n",
            "Epoch 359/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0126 - mean_absolute_error: 0.0126 - val_loss: 0.0174 - val_mean_absolute_error: 0.0174\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.00611\n",
            "Epoch 360/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0097 - mean_absolute_error: 0.0097 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.00611\n",
            "Epoch 361/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0098 - mean_absolute_error: 0.0098 - val_loss: 0.0111 - val_mean_absolute_error: 0.0111\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.00611\n",
            "Epoch 362/1000\n",
            "388/388 [==============================] - 0s 162us/step - loss: 0.0083 - mean_absolute_error: 0.0083 - val_loss: 0.0167 - val_mean_absolute_error: 0.0167\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.00611\n",
            "Epoch 363/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0110 - val_mean_absolute_error: 0.0110\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.00611\n",
            "Epoch 364/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0074 - mean_absolute_error: 0.0074 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.00611\n",
            "Epoch 365/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0147 - val_mean_absolute_error: 0.0147\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.00611\n",
            "Epoch 366/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0097 - mean_absolute_error: 0.0097 - val_loss: 0.0165 - val_mean_absolute_error: 0.0165\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.00611\n",
            "Epoch 367/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0073 - mean_absolute_error: 0.0073 - val_loss: 0.0081 - val_mean_absolute_error: 0.0081\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.00611\n",
            "Epoch 368/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0085 - mean_absolute_error: 0.0085 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.00611\n",
            "Epoch 369/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0077 - mean_absolute_error: 0.0077 - val_loss: 0.0111 - val_mean_absolute_error: 0.0111\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.00611\n",
            "Epoch 370/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0082 - mean_absolute_error: 0.0082 - val_loss: 0.0108 - val_mean_absolute_error: 0.0108\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.00611\n",
            "Epoch 371/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0078 - mean_absolute_error: 0.0078 - val_loss: 0.0076 - val_mean_absolute_error: 0.0076\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.00611\n",
            "Epoch 372/1000\n",
            "388/388 [==============================] - 0s 146us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0107 - val_mean_absolute_error: 0.0107\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.00611\n",
            "Epoch 373/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0068 - mean_absolute_error: 0.0068 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.00611\n",
            "Epoch 374/1000\n",
            "388/388 [==============================] - 0s 151us/step - loss: 0.0079 - mean_absolute_error: 0.0079 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.00611\n",
            "Epoch 375/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.00611\n",
            "Epoch 376/1000\n",
            "388/388 [==============================] - 0s 159us/step - loss: 0.0072 - mean_absolute_error: 0.0072 - val_loss: 0.0169 - val_mean_absolute_error: 0.0169\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.00611\n",
            "Epoch 377/1000\n",
            "388/388 [==============================] - 0s 151us/step - loss: 0.0127 - mean_absolute_error: 0.0127 - val_loss: 0.0266 - val_mean_absolute_error: 0.0266\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.00611\n",
            "Epoch 378/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0197 - mean_absolute_error: 0.0197 - val_loss: 0.0214 - val_mean_absolute_error: 0.0214\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.00611\n",
            "Epoch 379/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0117 - mean_absolute_error: 0.0117 - val_loss: 0.0172 - val_mean_absolute_error: 0.0172\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.00611\n",
            "Epoch 380/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0099 - mean_absolute_error: 0.0099 - val_loss: 0.0150 - val_mean_absolute_error: 0.0150\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.00611\n",
            "Epoch 381/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0168 - val_mean_absolute_error: 0.0168\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.00611\n",
            "Epoch 382/1000\n",
            "388/388 [==============================] - 0s 145us/step - loss: 0.0100 - mean_absolute_error: 0.0100 - val_loss: 0.0180 - val_mean_absolute_error: 0.0180\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.00611\n",
            "Epoch 383/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0062 - val_mean_absolute_error: 0.0062\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.00611\n",
            "Epoch 384/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0086 - mean_absolute_error: 0.0086 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.00611\n",
            "Epoch 385/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0114 - mean_absolute_error: 0.0114 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.00611\n",
            "Epoch 386/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0068 - mean_absolute_error: 0.0068 - val_loss: 0.0235 - val_mean_absolute_error: 0.0235\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.00611\n",
            "Epoch 387/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0231 - mean_absolute_error: 0.0231 - val_loss: 0.0201 - val_mean_absolute_error: 0.0201\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.00611\n",
            "Epoch 388/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0156 - mean_absolute_error: 0.0156 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.00611\n",
            "Epoch 389/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0111 - mean_absolute_error: 0.0111 - val_loss: 0.0217 - val_mean_absolute_error: 0.0217\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.00611\n",
            "Epoch 390/1000\n",
            "388/388 [==============================] - 0s 146us/step - loss: 0.0215 - mean_absolute_error: 0.0215 - val_loss: 0.0317 - val_mean_absolute_error: 0.0317\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.00611\n",
            "Epoch 391/1000\n",
            "388/388 [==============================] - 0s 145us/step - loss: 0.0159 - mean_absolute_error: 0.0159 - val_loss: 0.0213 - val_mean_absolute_error: 0.0213\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.00611\n",
            "Epoch 392/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0153 - mean_absolute_error: 0.0153 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.00611\n",
            "Epoch 393/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0116 - mean_absolute_error: 0.0116 - val_loss: 0.0185 - val_mean_absolute_error: 0.0185\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.00611\n",
            "Epoch 394/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.0197 - val_mean_absolute_error: 0.0197\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.00611\n",
            "Epoch 395/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0185 - mean_absolute_error: 0.0185 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.00611\n",
            "Epoch 396/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0161 - mean_absolute_error: 0.0161 - val_loss: 0.0256 - val_mean_absolute_error: 0.0256\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.00611\n",
            "Epoch 397/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0110 - mean_absolute_error: 0.0110 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.00611\n",
            "Epoch 398/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0066 - mean_absolute_error: 0.0066 - val_loss: 0.0235 - val_mean_absolute_error: 0.0235\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.00611\n",
            "Epoch 399/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0210 - mean_absolute_error: 0.0210 - val_loss: 0.0351 - val_mean_absolute_error: 0.0351\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.00611\n",
            "Epoch 400/1000\n",
            "388/388 [==============================] - 0s 149us/step - loss: 0.0191 - mean_absolute_error: 0.0191 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.00611\n",
            "Epoch 401/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0123 - mean_absolute_error: 0.0123 - val_loss: 0.0168 - val_mean_absolute_error: 0.0168\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.00611\n",
            "Epoch 402/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0144 - mean_absolute_error: 0.0144 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.00611\n",
            "Epoch 403/1000\n",
            "388/388 [==============================] - 0s 152us/step - loss: 0.0068 - mean_absolute_error: 0.0068 - val_loss: 0.0063 - val_mean_absolute_error: 0.0063\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.00611\n",
            "Epoch 404/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0071 - mean_absolute_error: 0.0071 - val_loss: 0.0083 - val_mean_absolute_error: 0.0083\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.00611\n",
            "Epoch 405/1000\n",
            "388/388 [==============================] - 0s 157us/step - loss: 0.0071 - mean_absolute_error: 0.0071 - val_loss: 0.0074 - val_mean_absolute_error: 0.0074\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.00611\n",
            "Epoch 406/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.00611\n",
            "Epoch 407/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0070 - mean_absolute_error: 0.0070 - val_loss: 0.0081 - val_mean_absolute_error: 0.0081\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.00611\n",
            "Epoch 408/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0115 - mean_absolute_error: 0.0115 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.00611\n",
            "Epoch 409/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0120 - mean_absolute_error: 0.0120 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.00611\n",
            "Epoch 410/1000\n",
            "388/388 [==============================] - 0s 151us/step - loss: 0.0108 - mean_absolute_error: 0.0108 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.00611\n",
            "Epoch 411/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0098 - mean_absolute_error: 0.0098 - val_loss: 0.0216 - val_mean_absolute_error: 0.0216\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.00611\n",
            "Epoch 412/1000\n",
            "388/388 [==============================] - 0s 149us/step - loss: 0.0205 - mean_absolute_error: 0.0205 - val_loss: 0.0209 - val_mean_absolute_error: 0.0209\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.00611\n",
            "Epoch 413/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0123 - mean_absolute_error: 0.0123 - val_loss: 0.0086 - val_mean_absolute_error: 0.0086\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.00611\n",
            "Epoch 414/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0057 - mean_absolute_error: 0.0057 - val_loss: 0.0075 - val_mean_absolute_error: 0.0075\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.00611\n",
            "Epoch 415/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0063 - mean_absolute_error: 0.0063 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.00611\n",
            "Epoch 416/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0087 - mean_absolute_error: 0.0087 - val_loss: 0.0109 - val_mean_absolute_error: 0.0109\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.00611\n",
            "Epoch 417/1000\n",
            "388/388 [==============================] - 0s 146us/step - loss: 0.0057 - mean_absolute_error: 0.0057 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.00611\n",
            "Epoch 418/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0083 - val_mean_absolute_error: 0.0083\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.00611\n",
            "Epoch 419/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0048 - mean_absolute_error: 0.0048 - val_loss: 0.0072 - val_mean_absolute_error: 0.0072\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.00611\n",
            "Epoch 420/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0067 - val_mean_absolute_error: 0.0067\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.00611\n",
            "Epoch 421/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0037 - mean_absolute_error: 0.0037 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.00611\n",
            "Epoch 422/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0071 - mean_absolute_error: 0.0071 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.00611\n",
            "Epoch 423/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0074 - val_mean_absolute_error: 0.0074\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.00611\n",
            "Epoch 424/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0065 - val_mean_absolute_error: 0.0065\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.00611\n",
            "Epoch 425/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0078 - val_mean_absolute_error: 0.0078\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.00611\n",
            "Epoch 426/1000\n",
            "388/388 [==============================] - 0s 125us/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0071 - val_mean_absolute_error: 0.0071\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.00611\n",
            "Epoch 427/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0156 - val_mean_absolute_error: 0.0156\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.00611\n",
            "Epoch 428/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0072 - mean_absolute_error: 0.0072 - val_loss: 0.0068 - val_mean_absolute_error: 0.0068\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.00611\n",
            "Epoch 429/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0069 - mean_absolute_error: 0.0069 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.00611\n",
            "Epoch 430/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0219 - val_mean_absolute_error: 0.0219\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.00611\n",
            "Epoch 431/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0107 - mean_absolute_error: 0.0107 - val_loss: 0.0070 - val_mean_absolute_error: 0.0070\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.00611\n",
            "Epoch 432/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0054 - mean_absolute_error: 0.0054 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.00611\n",
            "Epoch 433/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.00611\n",
            "Epoch 434/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0053 - mean_absolute_error: 0.0053 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.00611\n",
            "Epoch 435/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0065 - mean_absolute_error: 0.0065 - val_loss: 0.0071 - val_mean_absolute_error: 0.0071\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.00611\n",
            "Epoch 436/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0036 - mean_absolute_error: 0.0036 - val_loss: 0.0071 - val_mean_absolute_error: 0.0071\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.00611\n",
            "Epoch 437/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0067 - val_mean_absolute_error: 0.0067\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.00611\n",
            "Epoch 438/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0036 - mean_absolute_error: 0.0036 - val_loss: 0.0059 - val_mean_absolute_error: 0.0059\n",
            "\n",
            "Epoch 00438: val_loss improved from 0.00611 to 0.00593, saving model to Weights-438--0.00593.hdf5\n",
            "Epoch 439/1000\n",
            "388/388 [==============================] - 0s 163us/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0074 - val_mean_absolute_error: 0.0074\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.00593\n",
            "Epoch 440/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.00593\n",
            "Epoch 441/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0059 - mean_absolute_error: 0.0059 - val_loss: 0.0073 - val_mean_absolute_error: 0.0073\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.00593\n",
            "Epoch 442/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0051 - mean_absolute_error: 0.0051 - val_loss: 0.0070 - val_mean_absolute_error: 0.0070\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.00593\n",
            "Epoch 443/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0078 - mean_absolute_error: 0.0078 - val_loss: 0.0157 - val_mean_absolute_error: 0.0157\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.00593\n",
            "Epoch 444/1000\n",
            "388/388 [==============================] - 0s 162us/step - loss: 0.0103 - mean_absolute_error: 0.0103 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.00593\n",
            "Epoch 445/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0136 - mean_absolute_error: 0.0136 - val_loss: 0.0161 - val_mean_absolute_error: 0.0161\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.00593\n",
            "Epoch 446/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0133 - mean_absolute_error: 0.0133 - val_loss: 0.0176 - val_mean_absolute_error: 0.0176\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.00593\n",
            "Epoch 447/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0164 - mean_absolute_error: 0.0164 - val_loss: 0.0140 - val_mean_absolute_error: 0.0140\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.00593\n",
            "Epoch 448/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0078 - mean_absolute_error: 0.0078 - val_loss: 0.0156 - val_mean_absolute_error: 0.0156\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.00593\n",
            "Epoch 449/1000\n",
            "388/388 [==============================] - 0s 153us/step - loss: 0.0122 - mean_absolute_error: 0.0122 - val_loss: 0.0145 - val_mean_absolute_error: 0.0145\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.00593\n",
            "Epoch 450/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0106 - mean_absolute_error: 0.0106 - val_loss: 0.0148 - val_mean_absolute_error: 0.0148\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.00593\n",
            "Epoch 451/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.00593\n",
            "Epoch 452/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0061 - mean_absolute_error: 0.0061 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.00593\n",
            "Epoch 453/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0073 - mean_absolute_error: 0.0073 - val_loss: 0.0065 - val_mean_absolute_error: 0.0065\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.00593\n",
            "Epoch 454/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0073 - val_mean_absolute_error: 0.0073\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.00593\n",
            "Epoch 455/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0053 - mean_absolute_error: 0.0053 - val_loss: 0.0055 - val_mean_absolute_error: 0.0055\n",
            "\n",
            "Epoch 00455: val_loss improved from 0.00593 to 0.00550, saving model to Weights-455--0.00550.hdf5\n",
            "Epoch 456/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0207 - val_mean_absolute_error: 0.0207\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.00550\n",
            "Epoch 457/1000\n",
            "388/388 [==============================] - 0s 149us/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.00550\n",
            "Epoch 458/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0053 - mean_absolute_error: 0.0053 - val_loss: 0.0105 - val_mean_absolute_error: 0.0105\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.00550\n",
            "Epoch 459/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0054 - mean_absolute_error: 0.0054 - val_loss: 0.0078 - val_mean_absolute_error: 0.0078\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.00550\n",
            "Epoch 460/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0058 - mean_absolute_error: 0.0058 - val_loss: 0.0098 - val_mean_absolute_error: 0.0098\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.00550\n",
            "Epoch 461/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0071 - mean_absolute_error: 0.0071 - val_loss: 0.0064 - val_mean_absolute_error: 0.0064\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.00550\n",
            "Epoch 462/1000\n",
            "388/388 [==============================] - 0s 157us/step - loss: 0.0083 - mean_absolute_error: 0.0083 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.00550\n",
            "Epoch 463/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.00550\n",
            "Epoch 464/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0080 - mean_absolute_error: 0.0080 - val_loss: 0.0253 - val_mean_absolute_error: 0.0253\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.00550\n",
            "Epoch 465/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0122 - mean_absolute_error: 0.0122 - val_loss: 0.0072 - val_mean_absolute_error: 0.0072\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.00550\n",
            "Epoch 466/1000\n",
            "388/388 [==============================] - 0s 207us/step - loss: 0.0052 - mean_absolute_error: 0.0052 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.00550\n",
            "Epoch 467/1000\n",
            "388/388 [==============================] - 0s 157us/step - loss: 0.0072 - mean_absolute_error: 0.0072 - val_loss: 0.0150 - val_mean_absolute_error: 0.0150\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.00550\n",
            "Epoch 468/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.00550\n",
            "Epoch 469/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0078 - mean_absolute_error: 0.0078 - val_loss: 0.0077 - val_mean_absolute_error: 0.0077\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.00550\n",
            "Epoch 470/1000\n",
            "388/388 [==============================] - 0s 148us/step - loss: 0.0058 - mean_absolute_error: 0.0058 - val_loss: 0.0081 - val_mean_absolute_error: 0.0081\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.00550\n",
            "Epoch 471/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0049 - mean_absolute_error: 0.0049 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.00550\n",
            "Epoch 472/1000\n",
            "388/388 [==============================] - 0s 152us/step - loss: 0.0070 - mean_absolute_error: 0.0070 - val_loss: 0.0154 - val_mean_absolute_error: 0.0154\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.00550\n",
            "Epoch 473/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0080 - mean_absolute_error: 0.0080 - val_loss: 0.0063 - val_mean_absolute_error: 0.0063\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.00550\n",
            "Epoch 474/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0058 - val_mean_absolute_error: 0.0058\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.00550\n",
            "Epoch 475/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0037 - mean_absolute_error: 0.0037 - val_loss: 0.0083 - val_mean_absolute_error: 0.0083\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.00550\n",
            "Epoch 476/1000\n",
            "388/388 [==============================] - 0s 150us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0066 - val_mean_absolute_error: 0.0066\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.00550\n",
            "Epoch 477/1000\n",
            "388/388 [==============================] - 0s 150us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.00550\n",
            "Epoch 478/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0079 - mean_absolute_error: 0.0079 - val_loss: 0.0086 - val_mean_absolute_error: 0.0086\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.00550\n",
            "Epoch 479/1000\n",
            "388/388 [==============================] - 0s 146us/step - loss: 0.0075 - mean_absolute_error: 0.0075 - val_loss: 0.0192 - val_mean_absolute_error: 0.0192\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.00550\n",
            "Epoch 480/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0093 - mean_absolute_error: 0.0093 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.00550\n",
            "Epoch 481/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0066 - mean_absolute_error: 0.0066 - val_loss: 0.0067 - val_mean_absolute_error: 0.0067\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.00550\n",
            "Epoch 482/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0052 - mean_absolute_error: 0.0052 - val_loss: 0.0070 - val_mean_absolute_error: 0.0070\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.00550\n",
            "Epoch 483/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0072 - mean_absolute_error: 0.0072 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.00550\n",
            "Epoch 484/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0099 - mean_absolute_error: 0.0099 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.00550\n",
            "Epoch 485/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0072 - mean_absolute_error: 0.0072 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.00550\n",
            "Epoch 486/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0071 - mean_absolute_error: 0.0071 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.00550\n",
            "Epoch 487/1000\n",
            "388/388 [==============================] - 0s 154us/step - loss: 0.0060 - mean_absolute_error: 0.0060 - val_loss: 0.0098 - val_mean_absolute_error: 0.0098\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.00550\n",
            "Epoch 488/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0067 - mean_absolute_error: 0.0067 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.00550\n",
            "Epoch 489/1000\n",
            "388/388 [==============================] - 0s 154us/step - loss: 0.0036 - mean_absolute_error: 0.0036 - val_loss: 0.0107 - val_mean_absolute_error: 0.0107\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.00550\n",
            "Epoch 490/1000\n",
            "388/388 [==============================] - 0s 164us/step - loss: 0.0077 - mean_absolute_error: 0.0077 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.00550\n",
            "Epoch 491/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.00550\n",
            "Epoch 492/1000\n",
            "388/388 [==============================] - 0s 155us/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0101 - val_mean_absolute_error: 0.0101\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.00550\n",
            "Epoch 493/1000\n",
            "388/388 [==============================] - 0s 145us/step - loss: 0.0087 - mean_absolute_error: 0.0087 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.00550\n",
            "Epoch 494/1000\n",
            "388/388 [==============================] - 0s 149us/step - loss: 0.0051 - mean_absolute_error: 0.0051 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.00550\n",
            "Epoch 495/1000\n",
            "388/388 [==============================] - 0s 148us/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.00550\n",
            "Epoch 496/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0158 - val_mean_absolute_error: 0.0158\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.00550\n",
            "Epoch 497/1000\n",
            "388/388 [==============================] - 0s 149us/step - loss: 0.0076 - mean_absolute_error: 0.0076 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.00550\n",
            "Epoch 498/1000\n",
            "388/388 [==============================] - 0s 153us/step - loss: 0.0107 - mean_absolute_error: 0.0107 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.00550\n",
            "Epoch 499/1000\n",
            "388/388 [==============================] - 0s 181us/step - loss: 0.0053 - mean_absolute_error: 0.0053 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.00550\n",
            "Epoch 500/1000\n",
            "388/388 [==============================] - 0s 149us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0105 - val_mean_absolute_error: 0.0105\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.00550\n",
            "Epoch 501/1000\n",
            "388/388 [==============================] - 0s 151us/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097\n",
            "\n",
            "Epoch 00501: val_loss did not improve from 0.00550\n",
            "Epoch 502/1000\n",
            "388/388 [==============================] - 0s 150us/step - loss: 0.0051 - mean_absolute_error: 0.0051 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097\n",
            "\n",
            "Epoch 00502: val_loss did not improve from 0.00550\n",
            "Epoch 503/1000\n",
            "388/388 [==============================] - 0s 166us/step - loss: 0.0058 - mean_absolute_error: 0.0058 - val_loss: 0.0086 - val_mean_absolute_error: 0.0086\n",
            "\n",
            "Epoch 00503: val_loss did not improve from 0.00550\n",
            "Epoch 504/1000\n",
            "388/388 [==============================] - 0s 154us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0071 - val_mean_absolute_error: 0.0071\n",
            "\n",
            "Epoch 00504: val_loss did not improve from 0.00550\n",
            "Epoch 505/1000\n",
            "388/388 [==============================] - 0s 153us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0140 - val_mean_absolute_error: 0.0140\n",
            "\n",
            "Epoch 00505: val_loss did not improve from 0.00550\n",
            "Epoch 506/1000\n",
            "388/388 [==============================] - 0s 154us/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0077 - val_mean_absolute_error: 0.0077\n",
            "\n",
            "Epoch 00506: val_loss did not improve from 0.00550\n",
            "Epoch 507/1000\n",
            "388/388 [==============================] - 0s 161us/step - loss: 0.0086 - mean_absolute_error: 0.0086 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106\n",
            "\n",
            "Epoch 00507: val_loss did not improve from 0.00550\n",
            "Epoch 508/1000\n",
            "388/388 [==============================] - 0s 151us/step - loss: 0.0072 - mean_absolute_error: 0.0072 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085\n",
            "\n",
            "Epoch 00508: val_loss did not improve from 0.00550\n",
            "Epoch 509/1000\n",
            "388/388 [==============================] - 0s 159us/step - loss: 0.0052 - mean_absolute_error: 0.0052 - val_loss: 0.0060 - val_mean_absolute_error: 0.0060\n",
            "\n",
            "Epoch 00509: val_loss did not improve from 0.00550\n",
            "Epoch 510/1000\n",
            "388/388 [==============================] - 0s 155us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0074 - val_mean_absolute_error: 0.0074\n",
            "\n",
            "Epoch 00510: val_loss did not improve from 0.00550\n",
            "Epoch 511/1000\n",
            "388/388 [==============================] - 0s 158us/step - loss: 0.0045 - mean_absolute_error: 0.0045 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097\n",
            "\n",
            "Epoch 00511: val_loss did not improve from 0.00550\n",
            "Epoch 512/1000\n",
            "388/388 [==============================] - 0s 149us/step - loss: 0.0106 - mean_absolute_error: 0.0106 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134\n",
            "\n",
            "Epoch 00512: val_loss did not improve from 0.00550\n",
            "Epoch 513/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0095 - mean_absolute_error: 0.0095 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088\n",
            "\n",
            "Epoch 00513: val_loss did not improve from 0.00550\n",
            "Epoch 514/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0060 - mean_absolute_error: 0.0060 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085\n",
            "\n",
            "Epoch 00514: val_loss did not improve from 0.00550\n",
            "Epoch 515/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0052 - mean_absolute_error: 0.0052 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069\n",
            "\n",
            "Epoch 00515: val_loss did not improve from 0.00550\n",
            "Epoch 516/1000\n",
            "388/388 [==============================] - 0s 145us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094\n",
            "\n",
            "Epoch 00516: val_loss did not improve from 0.00550\n",
            "Epoch 517/1000\n",
            "388/388 [==============================] - 0s 146us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0086 - val_mean_absolute_error: 0.0086\n",
            "\n",
            "Epoch 00517: val_loss did not improve from 0.00550\n",
            "Epoch 518/1000\n",
            "388/388 [==============================] - 0s 149us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0063 - val_mean_absolute_error: 0.0063\n",
            "\n",
            "Epoch 00518: val_loss did not improve from 0.00550\n",
            "Epoch 519/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0036 - mean_absolute_error: 0.0036 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093\n",
            "\n",
            "Epoch 00519: val_loss did not improve from 0.00550\n",
            "Epoch 520/1000\n",
            "388/388 [==============================] - 0s 146us/step - loss: 0.0051 - mean_absolute_error: 0.0051 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122\n",
            "\n",
            "Epoch 00520: val_loss did not improve from 0.00550\n",
            "Epoch 521/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0077 - mean_absolute_error: 0.0077 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093\n",
            "\n",
            "Epoch 00521: val_loss did not improve from 0.00550\n",
            "Epoch 522/1000\n",
            "388/388 [==============================] - 0s 151us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0064 - val_mean_absolute_error: 0.0064\n",
            "\n",
            "Epoch 00522: val_loss did not improve from 0.00550\n",
            "Epoch 523/1000\n",
            "388/388 [==============================] - 0s 146us/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0086 - val_mean_absolute_error: 0.0086\n",
            "\n",
            "Epoch 00523: val_loss did not improve from 0.00550\n",
            "Epoch 524/1000\n",
            "388/388 [==============================] - 0s 148us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125\n",
            "\n",
            "Epoch 00524: val_loss did not improve from 0.00550\n",
            "Epoch 525/1000\n",
            "388/388 [==============================] - 0s 148us/step - loss: 0.0071 - mean_absolute_error: 0.0071 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
            "\n",
            "Epoch 00525: val_loss did not improve from 0.00550\n",
            "Epoch 526/1000\n",
            "388/388 [==============================] - 0s 151us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085\n",
            "\n",
            "Epoch 00526: val_loss did not improve from 0.00550\n",
            "Epoch 527/1000\n",
            "388/388 [==============================] - 0s 183us/step - loss: 0.0057 - mean_absolute_error: 0.0057 - val_loss: 0.0059 - val_mean_absolute_error: 0.0059\n",
            "\n",
            "Epoch 00527: val_loss did not improve from 0.00550\n",
            "Epoch 528/1000\n",
            "388/388 [==============================] - 0s 156us/step - loss: 0.0059 - mean_absolute_error: 0.0059 - val_loss: 0.0114 - val_mean_absolute_error: 0.0114\n",
            "\n",
            "Epoch 00528: val_loss did not improve from 0.00550\n",
            "Epoch 529/1000\n",
            "388/388 [==============================] - 0s 151us/step - loss: 0.0068 - mean_absolute_error: 0.0068 - val_loss: 0.0168 - val_mean_absolute_error: 0.0168\n",
            "\n",
            "Epoch 00529: val_loss did not improve from 0.00550\n",
            "Epoch 530/1000\n",
            "388/388 [==============================] - 0s 148us/step - loss: 0.0155 - mean_absolute_error: 0.0155 - val_loss: 0.0077 - val_mean_absolute_error: 0.0077\n",
            "\n",
            "Epoch 00530: val_loss did not improve from 0.00550\n",
            "Epoch 531/1000\n",
            "388/388 [==============================] - 0s 156us/step - loss: 0.0068 - mean_absolute_error: 0.0068 - val_loss: 0.0164 - val_mean_absolute_error: 0.0164\n",
            "\n",
            "Epoch 00531: val_loss did not improve from 0.00550\n",
            "Epoch 532/1000\n",
            "388/388 [==============================] - 0s 165us/step - loss: 0.0147 - mean_absolute_error: 0.0147 - val_loss: 0.0079 - val_mean_absolute_error: 0.0079\n",
            "\n",
            "Epoch 00532: val_loss did not improve from 0.00550\n",
            "Epoch 533/1000\n",
            "388/388 [==============================] - 0s 148us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0108 - val_mean_absolute_error: 0.0108\n",
            "\n",
            "Epoch 00533: val_loss did not improve from 0.00550\n",
            "Epoch 534/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0051 - mean_absolute_error: 0.0051 - val_loss: 0.0059 - val_mean_absolute_error: 0.0059\n",
            "\n",
            "Epoch 00534: val_loss did not improve from 0.00550\n",
            "Epoch 535/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087\n",
            "\n",
            "Epoch 00535: val_loss did not improve from 0.00550\n",
            "Epoch 536/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0056 - mean_absolute_error: 0.0056 - val_loss: 0.0072 - val_mean_absolute_error: 0.0072\n",
            "\n",
            "Epoch 00536: val_loss did not improve from 0.00550\n",
            "Epoch 537/1000\n",
            "388/388 [==============================] - 0s 146us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0068 - val_mean_absolute_error: 0.0068\n",
            "\n",
            "Epoch 00537: val_loss did not improve from 0.00550\n",
            "Epoch 538/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
            "\n",
            "Epoch 00538: val_loss did not improve from 0.00550\n",
            "Epoch 539/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0074 - mean_absolute_error: 0.0074 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104\n",
            "\n",
            "Epoch 00539: val_loss did not improve from 0.00550\n",
            "Epoch 540/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0074 - mean_absolute_error: 0.0074 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102\n",
            "\n",
            "Epoch 00540: val_loss did not improve from 0.00550\n",
            "Epoch 541/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0051 - mean_absolute_error: 0.0051 - val_loss: 0.0081 - val_mean_absolute_error: 0.0081\n",
            "\n",
            "Epoch 00541: val_loss did not improve from 0.00550\n",
            "Epoch 542/1000\n",
            "388/388 [==============================] - 0s 159us/step - loss: 0.0045 - mean_absolute_error: 0.0045 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
            "\n",
            "Epoch 00542: val_loss did not improve from 0.00550\n",
            "Epoch 543/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0078 - mean_absolute_error: 0.0078 - val_loss: 0.0081 - val_mean_absolute_error: 0.0081\n",
            "\n",
            "Epoch 00543: val_loss did not improve from 0.00550\n",
            "Epoch 544/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0057 - mean_absolute_error: 0.0057 - val_loss: 0.0080 - val_mean_absolute_error: 0.0080\n",
            "\n",
            "Epoch 00544: val_loss did not improve from 0.00550\n",
            "Epoch 545/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0061 - val_mean_absolute_error: 0.0061\n",
            "\n",
            "Epoch 00545: val_loss did not improve from 0.00550\n",
            "Epoch 546/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0036 - mean_absolute_error: 0.0036 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082\n",
            "\n",
            "Epoch 00546: val_loss did not improve from 0.00550\n",
            "Epoch 547/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0060 - mean_absolute_error: 0.0060 - val_loss: 0.0057 - val_mean_absolute_error: 0.0057\n",
            "\n",
            "Epoch 00547: val_loss did not improve from 0.00550\n",
            "Epoch 548/1000\n",
            "388/388 [==============================] - 0s 151us/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0065 - val_mean_absolute_error: 0.0065\n",
            "\n",
            "Epoch 00548: val_loss did not improve from 0.00550\n",
            "Epoch 549/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0031 - mean_absolute_error: 0.0031 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096\n",
            "\n",
            "Epoch 00549: val_loss did not improve from 0.00550\n",
            "Epoch 550/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106\n",
            "\n",
            "Epoch 00550: val_loss did not improve from 0.00550\n",
            "Epoch 551/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0059 - mean_absolute_error: 0.0059 - val_loss: 0.0227 - val_mean_absolute_error: 0.0227\n",
            "\n",
            "Epoch 00551: val_loss did not improve from 0.00550\n",
            "Epoch 552/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0214 - mean_absolute_error: 0.0214 - val_loss: 0.0292 - val_mean_absolute_error: 0.0292\n",
            "\n",
            "Epoch 00552: val_loss did not improve from 0.00550\n",
            "Epoch 553/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0167 - mean_absolute_error: 0.0167 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127\n",
            "\n",
            "Epoch 00553: val_loss did not improve from 0.00550\n",
            "Epoch 554/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0190 - mean_absolute_error: 0.0190 - val_loss: 0.0144 - val_mean_absolute_error: 0.0144\n",
            "\n",
            "Epoch 00554: val_loss did not improve from 0.00550\n",
            "Epoch 555/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0085 - mean_absolute_error: 0.0085 - val_loss: 0.0109 - val_mean_absolute_error: 0.0109\n",
            "\n",
            "Epoch 00555: val_loss did not improve from 0.00550\n",
            "Epoch 556/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0066 - mean_absolute_error: 0.0066 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106\n",
            "\n",
            "Epoch 00556: val_loss did not improve from 0.00550\n",
            "Epoch 557/1000\n",
            "388/388 [==============================] - 0s 149us/step - loss: 0.0057 - mean_absolute_error: 0.0057 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116\n",
            "\n",
            "Epoch 00557: val_loss did not improve from 0.00550\n",
            "Epoch 558/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0068 - mean_absolute_error: 0.0068 - val_loss: 0.0071 - val_mean_absolute_error: 0.0071\n",
            "\n",
            "Epoch 00558: val_loss did not improve from 0.00550\n",
            "Epoch 559/1000\n",
            "388/388 [==============================] - 0s 148us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089\n",
            "\n",
            "Epoch 00559: val_loss did not improve from 0.00550\n",
            "Epoch 560/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0062 - mean_absolute_error: 0.0062 - val_loss: 0.0084 - val_mean_absolute_error: 0.0084\n",
            "\n",
            "Epoch 00560: val_loss did not improve from 0.00550\n",
            "Epoch 561/1000\n",
            "388/388 [==============================] - 0s 164us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0062 - val_mean_absolute_error: 0.0062\n",
            "\n",
            "Epoch 00561: val_loss did not improve from 0.00550\n",
            "Epoch 562/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - val_loss: 0.0054 - val_mean_absolute_error: 0.0054\n",
            "\n",
            "Epoch 00562: val_loss improved from 0.00550 to 0.00537, saving model to Weights-562--0.00537.hdf5\n",
            "Epoch 563/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0056 - mean_absolute_error: 0.0056 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124\n",
            "\n",
            "Epoch 00563: val_loss did not improve from 0.00537\n",
            "Epoch 564/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0060 - mean_absolute_error: 0.0060 - val_loss: 0.0072 - val_mean_absolute_error: 0.0072\n",
            "\n",
            "Epoch 00564: val_loss did not improve from 0.00537\n",
            "Epoch 565/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0168 - val_mean_absolute_error: 0.0168\n",
            "\n",
            "Epoch 00565: val_loss did not improve from 0.00537\n",
            "Epoch 566/1000\n",
            "388/388 [==============================] - 0s 162us/step - loss: 0.0110 - mean_absolute_error: 0.0110 - val_loss: 0.0145 - val_mean_absolute_error: 0.0145\n",
            "\n",
            "Epoch 00566: val_loss did not improve from 0.00537\n",
            "Epoch 567/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0077 - mean_absolute_error: 0.0077 - val_loss: 0.0068 - val_mean_absolute_error: 0.0068\n",
            "\n",
            "Epoch 00567: val_loss did not improve from 0.00537\n",
            "Epoch 568/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0064 - mean_absolute_error: 0.0064 - val_loss: 0.0061 - val_mean_absolute_error: 0.0061\n",
            "\n",
            "Epoch 00568: val_loss did not improve from 0.00537\n",
            "Epoch 569/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0030 - mean_absolute_error: 0.0030 - val_loss: 0.0056 - val_mean_absolute_error: 0.0056\n",
            "\n",
            "Epoch 00569: val_loss did not improve from 0.00537\n",
            "Epoch 570/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - val_loss: 0.0101 - val_mean_absolute_error: 0.0101\n",
            "\n",
            "Epoch 00570: val_loss did not improve from 0.00537\n",
            "Epoch 571/1000\n",
            "388/388 [==============================] - 0s 149us/step - loss: 0.0067 - mean_absolute_error: 0.0067 - val_loss: 0.0066 - val_mean_absolute_error: 0.0066\n",
            "\n",
            "Epoch 00571: val_loss did not improve from 0.00537\n",
            "Epoch 572/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0072 - mean_absolute_error: 0.0072 - val_loss: 0.0068 - val_mean_absolute_error: 0.0068\n",
            "\n",
            "Epoch 00572: val_loss did not improve from 0.00537\n",
            "Epoch 573/1000\n",
            "388/388 [==============================] - 0s 146us/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0163 - val_mean_absolute_error: 0.0163\n",
            "\n",
            "Epoch 00573: val_loss did not improve from 0.00537\n",
            "Epoch 574/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0100 - mean_absolute_error: 0.0100 - val_loss: 0.0081 - val_mean_absolute_error: 0.0081\n",
            "\n",
            "Epoch 00574: val_loss did not improve from 0.00537\n",
            "Epoch 575/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0108 - val_mean_absolute_error: 0.0108\n",
            "\n",
            "Epoch 00575: val_loss did not improve from 0.00537\n",
            "Epoch 576/1000\n",
            "388/388 [==============================] - 0s 146us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0066 - val_mean_absolute_error: 0.0066\n",
            "\n",
            "Epoch 00576: val_loss did not improve from 0.00537\n",
            "Epoch 577/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0032 - mean_absolute_error: 0.0032 - val_loss: 0.0196 - val_mean_absolute_error: 0.0196\n",
            "\n",
            "Epoch 00577: val_loss did not improve from 0.00537\n",
            "Epoch 578/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0250 - mean_absolute_error: 0.0250 - val_loss: 0.0182 - val_mean_absolute_error: 0.0182\n",
            "\n",
            "Epoch 00578: val_loss did not improve from 0.00537\n",
            "Epoch 579/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0170 - mean_absolute_error: 0.0170 - val_loss: 0.0148 - val_mean_absolute_error: 0.0148\n",
            "\n",
            "Epoch 00579: val_loss did not improve from 0.00537\n",
            "Epoch 580/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0083 - mean_absolute_error: 0.0083 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129\n",
            "\n",
            "Epoch 00580: val_loss did not improve from 0.00537\n",
            "Epoch 581/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0093 - mean_absolute_error: 0.0093 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129\n",
            "\n",
            "Epoch 00581: val_loss did not improve from 0.00537\n",
            "Epoch 582/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0059 - mean_absolute_error: 0.0059 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126\n",
            "\n",
            "Epoch 00582: val_loss did not improve from 0.00537\n",
            "Epoch 583/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0069 - mean_absolute_error: 0.0069 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104\n",
            "\n",
            "Epoch 00583: val_loss did not improve from 0.00537\n",
            "Epoch 584/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0069 - mean_absolute_error: 0.0069 - val_loss: 0.0098 - val_mean_absolute_error: 0.0098\n",
            "\n",
            "Epoch 00584: val_loss did not improve from 0.00537\n",
            "Epoch 585/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0068 - val_mean_absolute_error: 0.0068\n",
            "\n",
            "Epoch 00585: val_loss did not improve from 0.00537\n",
            "Epoch 586/1000\n",
            "388/388 [==============================] - 0s 150us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099\n",
            "\n",
            "Epoch 00586: val_loss did not improve from 0.00537\n",
            "Epoch 587/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0068 - mean_absolute_error: 0.0068 - val_loss: 0.0074 - val_mean_absolute_error: 0.0074\n",
            "\n",
            "Epoch 00587: val_loss did not improve from 0.00537\n",
            "Epoch 588/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0057 - mean_absolute_error: 0.0057 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099\n",
            "\n",
            "Epoch 00588: val_loss did not improve from 0.00537\n",
            "Epoch 589/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0059 - mean_absolute_error: 0.0059 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099\n",
            "\n",
            "Epoch 00589: val_loss did not improve from 0.00537\n",
            "Epoch 590/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0056 - mean_absolute_error: 0.0056 - val_loss: 0.0061 - val_mean_absolute_error: 0.0061\n",
            "\n",
            "Epoch 00590: val_loss did not improve from 0.00537\n",
            "Epoch 591/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0084 - val_mean_absolute_error: 0.0084\n",
            "\n",
            "Epoch 00591: val_loss did not improve from 0.00537\n",
            "Epoch 592/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0072 - val_mean_absolute_error: 0.0072\n",
            "\n",
            "Epoch 00592: val_loss did not improve from 0.00537\n",
            "Epoch 593/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0062 - val_mean_absolute_error: 0.0062\n",
            "\n",
            "Epoch 00593: val_loss did not improve from 0.00537\n",
            "Epoch 594/1000\n",
            "388/388 [==============================] - 0s 156us/step - loss: 0.0051 - mean_absolute_error: 0.0051 - val_loss: 0.0146 - val_mean_absolute_error: 0.0146\n",
            "\n",
            "Epoch 00594: val_loss did not improve from 0.00537\n",
            "Epoch 595/1000\n",
            "388/388 [==============================] - 0s 152us/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0078 - val_mean_absolute_error: 0.0078\n",
            "\n",
            "Epoch 00595: val_loss did not improve from 0.00537\n",
            "Epoch 596/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106\n",
            "\n",
            "Epoch 00596: val_loss did not improve from 0.00537\n",
            "Epoch 597/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0068 - mean_absolute_error: 0.0068 - val_loss: 0.0076 - val_mean_absolute_error: 0.0076\n",
            "\n",
            "Epoch 00597: val_loss did not improve from 0.00537\n",
            "Epoch 598/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115\n",
            "\n",
            "Epoch 00598: val_loss did not improve from 0.00537\n",
            "Epoch 599/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0087 - mean_absolute_error: 0.0087 - val_loss: 0.0145 - val_mean_absolute_error: 0.0145\n",
            "\n",
            "Epoch 00599: val_loss did not improve from 0.00537\n",
            "Epoch 600/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0062 - mean_absolute_error: 0.0062 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096\n",
            "\n",
            "Epoch 00600: val_loss did not improve from 0.00537\n",
            "Epoch 601/1000\n",
            "388/388 [==============================] - 0s 159us/step - loss: 0.0048 - mean_absolute_error: 0.0048 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
            "\n",
            "Epoch 00601: val_loss did not improve from 0.00537\n",
            "Epoch 602/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0098 - mean_absolute_error: 0.0098 - val_loss: 0.0180 - val_mean_absolute_error: 0.0180\n",
            "\n",
            "Epoch 00602: val_loss did not improve from 0.00537\n",
            "Epoch 603/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0085 - mean_absolute_error: 0.0085 - val_loss: 0.0153 - val_mean_absolute_error: 0.0153\n",
            "\n",
            "Epoch 00603: val_loss did not improve from 0.00537\n",
            "Epoch 604/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0066 - val_mean_absolute_error: 0.0066\n",
            "\n",
            "Epoch 00604: val_loss did not improve from 0.00537\n",
            "Epoch 605/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123\n",
            "\n",
            "Epoch 00605: val_loss did not improve from 0.00537\n",
            "Epoch 606/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0052 - mean_absolute_error: 0.0052 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132\n",
            "\n",
            "Epoch 00606: val_loss did not improve from 0.00537\n",
            "Epoch 607/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0080 - mean_absolute_error: 0.0080 - val_loss: 0.0167 - val_mean_absolute_error: 0.0167\n",
            "\n",
            "Epoch 00607: val_loss did not improve from 0.00537\n",
            "Epoch 608/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0120 - mean_absolute_error: 0.0120 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134\n",
            "\n",
            "Epoch 00608: val_loss did not improve from 0.00537\n",
            "Epoch 609/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0082 - mean_absolute_error: 0.0082 - val_loss: 0.0059 - val_mean_absolute_error: 0.0059\n",
            "\n",
            "Epoch 00609: val_loss did not improve from 0.00537\n",
            "Epoch 610/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0070 - mean_absolute_error: 0.0070 - val_loss: 0.0068 - val_mean_absolute_error: 0.0068\n",
            "\n",
            "Epoch 00610: val_loss did not improve from 0.00537\n",
            "Epoch 611/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0066 - mean_absolute_error: 0.0066 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085\n",
            "\n",
            "Epoch 00611: val_loss did not improve from 0.00537\n",
            "Epoch 612/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115\n",
            "\n",
            "Epoch 00612: val_loss did not improve from 0.00537\n",
            "Epoch 613/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0054 - mean_absolute_error: 0.0054 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093\n",
            "\n",
            "Epoch 00613: val_loss did not improve from 0.00537\n",
            "Epoch 614/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0095 - val_mean_absolute_error: 0.0095\n",
            "\n",
            "Epoch 00614: val_loss did not improve from 0.00537\n",
            "Epoch 615/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0051 - mean_absolute_error: 0.0051 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104\n",
            "\n",
            "Epoch 00615: val_loss did not improve from 0.00537\n",
            "Epoch 616/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0073 - val_mean_absolute_error: 0.0073\n",
            "\n",
            "Epoch 00616: val_loss did not improve from 0.00537\n",
            "Epoch 617/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0062 - mean_absolute_error: 0.0062 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133\n",
            "\n",
            "Epoch 00617: val_loss did not improve from 0.00537\n",
            "Epoch 618/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0076 - mean_absolute_error: 0.0076 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087\n",
            "\n",
            "Epoch 00618: val_loss did not improve from 0.00537\n",
            "Epoch 619/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069\n",
            "\n",
            "Epoch 00619: val_loss did not improve from 0.00537\n",
            "Epoch 620/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0060 - val_mean_absolute_error: 0.0060\n",
            "\n",
            "Epoch 00620: val_loss did not improve from 0.00537\n",
            "Epoch 621/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - val_loss: 0.0080 - val_mean_absolute_error: 0.0080\n",
            "\n",
            "Epoch 00621: val_loss did not improve from 0.00537\n",
            "Epoch 622/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0056 - val_mean_absolute_error: 0.0056\n",
            "\n",
            "Epoch 00622: val_loss did not improve from 0.00537\n",
            "Epoch 623/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0048 - mean_absolute_error: 0.0048 - val_loss: 0.0165 - val_mean_absolute_error: 0.0165\n",
            "\n",
            "Epoch 00623: val_loss did not improve from 0.00537\n",
            "Epoch 624/1000\n",
            "388/388 [==============================] - 0s 125us/step - loss: 0.0153 - mean_absolute_error: 0.0153 - val_loss: 0.0159 - val_mean_absolute_error: 0.0159\n",
            "\n",
            "Epoch 00624: val_loss did not improve from 0.00537\n",
            "Epoch 625/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0054 - mean_absolute_error: 0.0054 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096\n",
            "\n",
            "Epoch 00625: val_loss did not improve from 0.00537\n",
            "Epoch 626/1000\n",
            "388/388 [==============================] - 0s 154us/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0073 - val_mean_absolute_error: 0.0073\n",
            "\n",
            "Epoch 00626: val_loss did not improve from 0.00537\n",
            "Epoch 627/1000\n",
            "388/388 [==============================] - 0s 157us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097\n",
            "\n",
            "Epoch 00627: val_loss did not improve from 0.00537\n",
            "Epoch 628/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0051 - mean_absolute_error: 0.0051 - val_loss: 0.0054 - val_mean_absolute_error: 0.0054\n",
            "\n",
            "Epoch 00628: val_loss did not improve from 0.00537\n",
            "Epoch 629/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087\n",
            "\n",
            "Epoch 00629: val_loss did not improve from 0.00537\n",
            "Epoch 630/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0101 - mean_absolute_error: 0.0101 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069\n",
            "\n",
            "Epoch 00630: val_loss did not improve from 0.00537\n",
            "Epoch 631/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0060 - mean_absolute_error: 0.0060 - val_loss: 0.0062 - val_mean_absolute_error: 0.0062\n",
            "\n",
            "Epoch 00631: val_loss did not improve from 0.00537\n",
            "Epoch 632/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0059 - val_mean_absolute_error: 0.0059\n",
            "\n",
            "Epoch 00632: val_loss did not improve from 0.00537\n",
            "Epoch 633/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0054 - val_mean_absolute_error: 0.0054\n",
            "\n",
            "Epoch 00633: val_loss improved from 0.00537 to 0.00536, saving model to Weights-633--0.00536.hdf5\n",
            "Epoch 634/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113\n",
            "\n",
            "Epoch 00634: val_loss did not improve from 0.00536\n",
            "Epoch 635/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0063 - val_mean_absolute_error: 0.0063\n",
            "\n",
            "Epoch 00635: val_loss did not improve from 0.00536\n",
            "Epoch 636/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0029 - mean_absolute_error: 0.0029 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118\n",
            "\n",
            "Epoch 00636: val_loss did not improve from 0.00536\n",
            "Epoch 637/1000\n",
            "388/388 [==============================] - 0s 155us/step - loss: 0.0061 - mean_absolute_error: 0.0061 - val_loss: 0.0053 - val_mean_absolute_error: 0.0053\n",
            "\n",
            "Epoch 00637: val_loss improved from 0.00536 to 0.00527, saving model to Weights-637--0.00527.hdf5\n",
            "Epoch 638/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0033 - mean_absolute_error: 0.0033 - val_loss: 0.0055 - val_mean_absolute_error: 0.0055\n",
            "\n",
            "Epoch 00638: val_loss did not improve from 0.00527\n",
            "Epoch 639/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0037 - mean_absolute_error: 0.0037 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085\n",
            "\n",
            "Epoch 00639: val_loss did not improve from 0.00527\n",
            "Epoch 640/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0033 - mean_absolute_error: 0.0033 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096\n",
            "\n",
            "Epoch 00640: val_loss did not improve from 0.00527\n",
            "Epoch 641/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0064 - val_mean_absolute_error: 0.0064\n",
            "\n",
            "Epoch 00641: val_loss did not improve from 0.00527\n",
            "Epoch 642/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0068 - val_mean_absolute_error: 0.0068\n",
            "\n",
            "Epoch 00642: val_loss did not improve from 0.00527\n",
            "Epoch 643/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0063 - mean_absolute_error: 0.0063 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116\n",
            "\n",
            "Epoch 00643: val_loss did not improve from 0.00527\n",
            "Epoch 644/1000\n",
            "388/388 [==============================] - 0s 217us/step - loss: 0.0070 - mean_absolute_error: 0.0070 - val_loss: 0.0190 - val_mean_absolute_error: 0.0190\n",
            "\n",
            "Epoch 00644: val_loss did not improve from 0.00527\n",
            "Epoch 645/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0158 - mean_absolute_error: 0.0158 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130\n",
            "\n",
            "Epoch 00645: val_loss did not improve from 0.00527\n",
            "Epoch 646/1000\n",
            "388/388 [==============================] - 0s 148us/step - loss: 0.0074 - mean_absolute_error: 0.0074 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100\n",
            "\n",
            "Epoch 00646: val_loss did not improve from 0.00527\n",
            "Epoch 647/1000\n",
            "388/388 [==============================] - 0s 148us/step - loss: 0.0053 - mean_absolute_error: 0.0053 - val_loss: 0.0053 - val_mean_absolute_error: 0.0053\n",
            "\n",
            "Epoch 00647: val_loss did not improve from 0.00527\n",
            "Epoch 648/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0057 - mean_absolute_error: 0.0057 - val_loss: 0.0080 - val_mean_absolute_error: 0.0080\n",
            "\n",
            "Epoch 00648: val_loss did not improve from 0.00527\n",
            "Epoch 649/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0060 - val_mean_absolute_error: 0.0060\n",
            "\n",
            "Epoch 00649: val_loss did not improve from 0.00527\n",
            "Epoch 650/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0035 - mean_absolute_error: 0.0035 - val_loss: 0.0065 - val_mean_absolute_error: 0.0065\n",
            "\n",
            "Epoch 00650: val_loss did not improve from 0.00527\n",
            "Epoch 651/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0055 - val_mean_absolute_error: 0.0055\n",
            "\n",
            "Epoch 00651: val_loss did not improve from 0.00527\n",
            "Epoch 652/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0031 - mean_absolute_error: 0.0031 - val_loss: 0.0140 - val_mean_absolute_error: 0.0140\n",
            "\n",
            "Epoch 00652: val_loss did not improve from 0.00527\n",
            "Epoch 653/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0096 - mean_absolute_error: 0.0096 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
            "\n",
            "Epoch 00653: val_loss did not improve from 0.00527\n",
            "Epoch 654/1000\n",
            "388/388 [==============================] - 0s 155us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0105 - val_mean_absolute_error: 0.0105\n",
            "\n",
            "Epoch 00654: val_loss did not improve from 0.00527\n",
            "Epoch 655/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0051 - mean_absolute_error: 0.0051 - val_loss: 0.0073 - val_mean_absolute_error: 0.0073\n",
            "\n",
            "Epoch 00655: val_loss did not improve from 0.00527\n",
            "Epoch 656/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0107 - val_mean_absolute_error: 0.0107\n",
            "\n",
            "Epoch 00656: val_loss did not improve from 0.00527\n",
            "Epoch 657/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0059 - mean_absolute_error: 0.0059 - val_loss: 0.0061 - val_mean_absolute_error: 0.0061\n",
            "\n",
            "Epoch 00657: val_loss did not improve from 0.00527\n",
            "Epoch 658/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - val_loss: 0.0076 - val_mean_absolute_error: 0.0076\n",
            "\n",
            "Epoch 00658: val_loss did not improve from 0.00527\n",
            "Epoch 659/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0142 - val_mean_absolute_error: 0.0142\n",
            "\n",
            "Epoch 00659: val_loss did not improve from 0.00527\n",
            "Epoch 660/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0077 - mean_absolute_error: 0.0077 - val_loss: 0.0108 - val_mean_absolute_error: 0.0108\n",
            "\n",
            "Epoch 00660: val_loss did not improve from 0.00527\n",
            "Epoch 661/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0064 - mean_absolute_error: 0.0064 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118\n",
            "\n",
            "Epoch 00661: val_loss did not improve from 0.00527\n",
            "Epoch 662/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0069 - mean_absolute_error: 0.0069 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113\n",
            "\n",
            "Epoch 00662: val_loss did not improve from 0.00527\n",
            "Epoch 663/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0057 - mean_absolute_error: 0.0057 - val_loss: 0.0080 - val_mean_absolute_error: 0.0080\n",
            "\n",
            "Epoch 00663: val_loss did not improve from 0.00527\n",
            "Epoch 664/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0062 - mean_absolute_error: 0.0062 - val_loss: 0.0147 - val_mean_absolute_error: 0.0147\n",
            "\n",
            "Epoch 00664: val_loss did not improve from 0.00527\n",
            "Epoch 665/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0085 - mean_absolute_error: 0.0085 - val_loss: 0.0054 - val_mean_absolute_error: 0.0054\n",
            "\n",
            "Epoch 00665: val_loss did not improve from 0.00527\n",
            "Epoch 666/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0058 - mean_absolute_error: 0.0058 - val_loss: 0.0214 - val_mean_absolute_error: 0.0214\n",
            "\n",
            "Epoch 00666: val_loss did not improve from 0.00527\n",
            "Epoch 667/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0176 - mean_absolute_error: 0.0176 - val_loss: 0.0152 - val_mean_absolute_error: 0.0152\n",
            "\n",
            "Epoch 00667: val_loss did not improve from 0.00527\n",
            "Epoch 668/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0104 - mean_absolute_error: 0.0104 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096\n",
            "\n",
            "Epoch 00668: val_loss did not improve from 0.00527\n",
            "Epoch 669/1000\n",
            "388/388 [==============================] - 0s 146us/step - loss: 0.0057 - mean_absolute_error: 0.0057 - val_loss: 0.0073 - val_mean_absolute_error: 0.0073\n",
            "\n",
            "Epoch 00669: val_loss did not improve from 0.00527\n",
            "Epoch 670/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0048 - mean_absolute_error: 0.0048 - val_loss: 0.0073 - val_mean_absolute_error: 0.0073\n",
            "\n",
            "Epoch 00670: val_loss did not improve from 0.00527\n",
            "Epoch 671/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133\n",
            "\n",
            "Epoch 00671: val_loss did not improve from 0.00527\n",
            "Epoch 672/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0085 - mean_absolute_error: 0.0085 - val_loss: 0.0086 - val_mean_absolute_error: 0.0086\n",
            "\n",
            "Epoch 00672: val_loss did not improve from 0.00527\n",
            "Epoch 673/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0071 - mean_absolute_error: 0.0071 - val_loss: 0.0135 - val_mean_absolute_error: 0.0135\n",
            "\n",
            "Epoch 00673: val_loss did not improve from 0.00527\n",
            "Epoch 674/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0083 - mean_absolute_error: 0.0083 - val_loss: 0.0105 - val_mean_absolute_error: 0.0105\n",
            "\n",
            "Epoch 00674: val_loss did not improve from 0.00527\n",
            "Epoch 675/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0049 - mean_absolute_error: 0.0049 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087\n",
            "\n",
            "Epoch 00675: val_loss did not improve from 0.00527\n",
            "Epoch 676/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0060 - val_mean_absolute_error: 0.0060\n",
            "\n",
            "Epoch 00676: val_loss did not improve from 0.00527\n",
            "Epoch 677/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069\n",
            "\n",
            "Epoch 00677: val_loss did not improve from 0.00527\n",
            "Epoch 678/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0058 - mean_absolute_error: 0.0058 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116\n",
            "\n",
            "Epoch 00678: val_loss did not improve from 0.00527\n",
            "Epoch 679/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0053 - mean_absolute_error: 0.0053 - val_loss: 0.0150 - val_mean_absolute_error: 0.0150\n",
            "\n",
            "Epoch 00679: val_loss did not improve from 0.00527\n",
            "Epoch 680/1000\n",
            "388/388 [==============================] - 0s 148us/step - loss: 0.0126 - mean_absolute_error: 0.0126 - val_loss: 0.0067 - val_mean_absolute_error: 0.0067\n",
            "\n",
            "Epoch 00680: val_loss did not improve from 0.00527\n",
            "Epoch 681/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0078 - mean_absolute_error: 0.0078 - val_loss: 0.0139 - val_mean_absolute_error: 0.0139\n",
            "\n",
            "Epoch 00681: val_loss did not improve from 0.00527\n",
            "Epoch 682/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0072 - mean_absolute_error: 0.0072 - val_loss: 0.0098 - val_mean_absolute_error: 0.0098\n",
            "\n",
            "Epoch 00682: val_loss did not improve from 0.00527\n",
            "Epoch 683/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0065 - val_mean_absolute_error: 0.0065\n",
            "\n",
            "Epoch 00683: val_loss did not improve from 0.00527\n",
            "Epoch 684/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0072 - mean_absolute_error: 0.0072 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119\n",
            "\n",
            "Epoch 00684: val_loss did not improve from 0.00527\n",
            "Epoch 685/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0158 - val_mean_absolute_error: 0.0158\n",
            "\n",
            "Epoch 00685: val_loss did not improve from 0.00527\n",
            "Epoch 686/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0179 - mean_absolute_error: 0.0179 - val_loss: 0.0197 - val_mean_absolute_error: 0.0197\n",
            "\n",
            "Epoch 00686: val_loss did not improve from 0.00527\n",
            "Epoch 687/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0111 - mean_absolute_error: 0.0111 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087\n",
            "\n",
            "Epoch 00687: val_loss did not improve from 0.00527\n",
            "Epoch 688/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0095 - mean_absolute_error: 0.0095 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106\n",
            "\n",
            "Epoch 00688: val_loss did not improve from 0.00527\n",
            "Epoch 689/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0075 - mean_absolute_error: 0.0075 - val_loss: 0.0080 - val_mean_absolute_error: 0.0080\n",
            "\n",
            "Epoch 00689: val_loss did not improve from 0.00527\n",
            "Epoch 690/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
            "\n",
            "Epoch 00690: val_loss did not improve from 0.00527\n",
            "Epoch 691/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0071 - mean_absolute_error: 0.0071 - val_loss: 0.0066 - val_mean_absolute_error: 0.0066\n",
            "\n",
            "Epoch 00691: val_loss did not improve from 0.00527\n",
            "Epoch 692/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082\n",
            "\n",
            "Epoch 00692: val_loss did not improve from 0.00527\n",
            "Epoch 693/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0070 - val_mean_absolute_error: 0.0070\n",
            "\n",
            "Epoch 00693: val_loss did not improve from 0.00527\n",
            "Epoch 694/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0037 - mean_absolute_error: 0.0037 - val_loss: 0.0098 - val_mean_absolute_error: 0.0098\n",
            "\n",
            "Epoch 00694: val_loss did not improve from 0.00527\n",
            "Epoch 695/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0050 - val_mean_absolute_error: 0.0050\n",
            "\n",
            "Epoch 00695: val_loss improved from 0.00527 to 0.00505, saving model to Weights-695--0.00505.hdf5\n",
            "Epoch 696/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0032 - mean_absolute_error: 0.0032 - val_loss: 0.0076 - val_mean_absolute_error: 0.0076\n",
            "\n",
            "Epoch 00696: val_loss did not improve from 0.00505\n",
            "Epoch 697/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0080 - val_mean_absolute_error: 0.0080\n",
            "\n",
            "Epoch 00697: val_loss did not improve from 0.00505\n",
            "Epoch 698/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0052 - mean_absolute_error: 0.0052 - val_loss: 0.0058 - val_mean_absolute_error: 0.0058\n",
            "\n",
            "Epoch 00698: val_loss did not improve from 0.00505\n",
            "Epoch 699/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0058 - val_mean_absolute_error: 0.0058\n",
            "\n",
            "Epoch 00699: val_loss did not improve from 0.00505\n",
            "Epoch 700/1000\n",
            "388/388 [==============================] - 0s 148us/step - loss: 0.0054 - mean_absolute_error: 0.0054 - val_loss: 0.0051 - val_mean_absolute_error: 0.0051\n",
            "\n",
            "Epoch 00700: val_loss did not improve from 0.00505\n",
            "Epoch 701/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0051 - mean_absolute_error: 0.0051 - val_loss: 0.0073 - val_mean_absolute_error: 0.0073\n",
            "\n",
            "Epoch 00701: val_loss did not improve from 0.00505\n",
            "Epoch 702/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097\n",
            "\n",
            "Epoch 00702: val_loss did not improve from 0.00505\n",
            "Epoch 703/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0087 - mean_absolute_error: 0.0087 - val_loss: 0.0105 - val_mean_absolute_error: 0.0105\n",
            "\n",
            "Epoch 00703: val_loss did not improve from 0.00505\n",
            "Epoch 704/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0074 - mean_absolute_error: 0.0074 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088\n",
            "\n",
            "Epoch 00704: val_loss did not improve from 0.00505\n",
            "Epoch 705/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106\n",
            "\n",
            "Epoch 00705: val_loss did not improve from 0.00505\n",
            "Epoch 706/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0061 - mean_absolute_error: 0.0061 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
            "\n",
            "Epoch 00706: val_loss did not improve from 0.00505\n",
            "Epoch 707/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0048 - mean_absolute_error: 0.0048 - val_loss: 0.0063 - val_mean_absolute_error: 0.0063\n",
            "\n",
            "Epoch 00707: val_loss did not improve from 0.00505\n",
            "Epoch 708/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087\n",
            "\n",
            "Epoch 00708: val_loss did not improve from 0.00505\n",
            "Epoch 709/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0035 - mean_absolute_error: 0.0035 - val_loss: 0.0054 - val_mean_absolute_error: 0.0054\n",
            "\n",
            "Epoch 00709: val_loss did not improve from 0.00505\n",
            "Epoch 710/1000\n",
            "388/388 [==============================] - 0s 153us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0065 - val_mean_absolute_error: 0.0065\n",
            "\n",
            "Epoch 00710: val_loss did not improve from 0.00505\n",
            "Epoch 711/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0062 - val_mean_absolute_error: 0.0062\n",
            "\n",
            "Epoch 00711: val_loss did not improve from 0.00505\n",
            "Epoch 712/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0067 - mean_absolute_error: 0.0067 - val_loss: 0.0067 - val_mean_absolute_error: 0.0067\n",
            "\n",
            "Epoch 00712: val_loss did not improve from 0.00505\n",
            "Epoch 713/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0076 - val_mean_absolute_error: 0.0076\n",
            "\n",
            "Epoch 00713: val_loss did not improve from 0.00505\n",
            "Epoch 714/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0049 - mean_absolute_error: 0.0049 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082\n",
            "\n",
            "Epoch 00714: val_loss did not improve from 0.00505\n",
            "Epoch 715/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0035 - mean_absolute_error: 0.0035 - val_loss: 0.0067 - val_mean_absolute_error: 0.0067\n",
            "\n",
            "Epoch 00715: val_loss did not improve from 0.00505\n",
            "Epoch 716/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0061 - val_mean_absolute_error: 0.0061\n",
            "\n",
            "Epoch 00716: val_loss did not improve from 0.00505\n",
            "Epoch 717/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0032 - mean_absolute_error: 0.0032 - val_loss: 0.0051 - val_mean_absolute_error: 0.0051\n",
            "\n",
            "Epoch 00717: val_loss did not improve from 0.00505\n",
            "Epoch 718/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121\n",
            "\n",
            "Epoch 00718: val_loss did not improve from 0.00505\n",
            "Epoch 719/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0080 - mean_absolute_error: 0.0080 - val_loss: 0.0080 - val_mean_absolute_error: 0.0080\n",
            "\n",
            "Epoch 00719: val_loss did not improve from 0.00505\n",
            "Epoch 720/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0151 - val_mean_absolute_error: 0.0151\n",
            "\n",
            "Epoch 00720: val_loss did not improve from 0.00505\n",
            "Epoch 721/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0107 - mean_absolute_error: 0.0107 - val_loss: 0.0146 - val_mean_absolute_error: 0.0146\n",
            "\n",
            "Epoch 00721: val_loss did not improve from 0.00505\n",
            "Epoch 722/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0066 - mean_absolute_error: 0.0066 - val_loss: 0.0178 - val_mean_absolute_error: 0.0178\n",
            "\n",
            "Epoch 00722: val_loss did not improve from 0.00505\n",
            "Epoch 723/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0121 - mean_absolute_error: 0.0121 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089\n",
            "\n",
            "Epoch 00723: val_loss did not improve from 0.00505\n",
            "Epoch 724/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0061 - mean_absolute_error: 0.0061 - val_loss: 0.0059 - val_mean_absolute_error: 0.0059\n",
            "\n",
            "Epoch 00724: val_loss did not improve from 0.00505\n",
            "Epoch 725/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0045 - mean_absolute_error: 0.0045 - val_loss: 0.0051 - val_mean_absolute_error: 0.0051\n",
            "\n",
            "Epoch 00725: val_loss did not improve from 0.00505\n",
            "Epoch 726/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082\n",
            "\n",
            "Epoch 00726: val_loss did not improve from 0.00505\n",
            "Epoch 727/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0080 - mean_absolute_error: 0.0080 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103\n",
            "\n",
            "Epoch 00727: val_loss did not improve from 0.00505\n",
            "Epoch 728/1000\n",
            "388/388 [==============================] - 0s 125us/step - loss: 0.0073 - mean_absolute_error: 0.0073 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088\n",
            "\n",
            "Epoch 00728: val_loss did not improve from 0.00505\n",
            "Epoch 729/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0067 - mean_absolute_error: 0.0067 - val_loss: 0.0075 - val_mean_absolute_error: 0.0075\n",
            "\n",
            "Epoch 00729: val_loss did not improve from 0.00505\n",
            "Epoch 730/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0053 - val_mean_absolute_error: 0.0053\n",
            "\n",
            "Epoch 00730: val_loss did not improve from 0.00505\n",
            "Epoch 731/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0035 - mean_absolute_error: 0.0035 - val_loss: 0.0058 - val_mean_absolute_error: 0.0058\n",
            "\n",
            "Epoch 00731: val_loss did not improve from 0.00505\n",
            "Epoch 732/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0031 - mean_absolute_error: 0.0031 - val_loss: 0.0057 - val_mean_absolute_error: 0.0057\n",
            "\n",
            "Epoch 00732: val_loss did not improve from 0.00505\n",
            "Epoch 733/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0031 - mean_absolute_error: 0.0031 - val_loss: 0.0109 - val_mean_absolute_error: 0.0109\n",
            "\n",
            "Epoch 00733: val_loss did not improve from 0.00505\n",
            "Epoch 734/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0218 - val_mean_absolute_error: 0.0218\n",
            "\n",
            "Epoch 00734: val_loss did not improve from 0.00505\n",
            "Epoch 735/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0111 - mean_absolute_error: 0.0111 - val_loss: 0.0169 - val_mean_absolute_error: 0.0169\n",
            "\n",
            "Epoch 00735: val_loss did not improve from 0.00505\n",
            "Epoch 736/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0060 - mean_absolute_error: 0.0060 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069\n",
            "\n",
            "Epoch 00736: val_loss did not improve from 0.00505\n",
            "Epoch 737/1000\n",
            "388/388 [==============================] - 0s 145us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0052 - val_mean_absolute_error: 0.0052\n",
            "\n",
            "Epoch 00737: val_loss did not improve from 0.00505\n",
            "Epoch 738/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0062 - mean_absolute_error: 0.0062 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089\n",
            "\n",
            "Epoch 00738: val_loss did not improve from 0.00505\n",
            "Epoch 739/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0082 - mean_absolute_error: 0.0082 - val_loss: 0.0218 - val_mean_absolute_error: 0.0218\n",
            "\n",
            "Epoch 00739: val_loss did not improve from 0.00505\n",
            "Epoch 740/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0209 - val_mean_absolute_error: 0.0209\n",
            "\n",
            "Epoch 00740: val_loss did not improve from 0.00505\n",
            "Epoch 741/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0293 - mean_absolute_error: 0.0293 - val_loss: 0.0242 - val_mean_absolute_error: 0.0242\n",
            "\n",
            "Epoch 00741: val_loss did not improve from 0.00505\n",
            "Epoch 742/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0207 - mean_absolute_error: 0.0207 - val_loss: 0.0181 - val_mean_absolute_error: 0.0181\n",
            "\n",
            "Epoch 00742: val_loss did not improve from 0.00505\n",
            "Epoch 743/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.0101 - val_mean_absolute_error: 0.0101\n",
            "\n",
            "Epoch 00743: val_loss did not improve from 0.00505\n",
            "Epoch 744/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0053 - mean_absolute_error: 0.0053 - val_loss: 0.0065 - val_mean_absolute_error: 0.0065\n",
            "\n",
            "Epoch 00744: val_loss did not improve from 0.00505\n",
            "Epoch 745/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0037 - mean_absolute_error: 0.0037 - val_loss: 0.0055 - val_mean_absolute_error: 0.0055\n",
            "\n",
            "Epoch 00745: val_loss did not improve from 0.00505\n",
            "Epoch 746/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - val_loss: 0.0083 - val_mean_absolute_error: 0.0083\n",
            "\n",
            "Epoch 00746: val_loss did not improve from 0.00505\n",
            "Epoch 747/1000\n",
            "388/388 [==============================] - 0s 125us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0060 - val_mean_absolute_error: 0.0060\n",
            "\n",
            "Epoch 00747: val_loss did not improve from 0.00505\n",
            "Epoch 748/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0145 - val_mean_absolute_error: 0.0145\n",
            "\n",
            "Epoch 00748: val_loss did not improve from 0.00505\n",
            "Epoch 749/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0061 - mean_absolute_error: 0.0061 - val_loss: 0.0053 - val_mean_absolute_error: 0.0053\n",
            "\n",
            "Epoch 00749: val_loss did not improve from 0.00505\n",
            "Epoch 750/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0068 - val_mean_absolute_error: 0.0068\n",
            "\n",
            "Epoch 00750: val_loss did not improve from 0.00505\n",
            "Epoch 751/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0037 - mean_absolute_error: 0.0037 - val_loss: 0.0078 - val_mean_absolute_error: 0.0078\n",
            "\n",
            "Epoch 00751: val_loss did not improve from 0.00505\n",
            "Epoch 752/1000\n",
            "388/388 [==============================] - 0s 125us/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0066 - val_mean_absolute_error: 0.0066\n",
            "\n",
            "Epoch 00752: val_loss did not improve from 0.00505\n",
            "Epoch 753/1000\n",
            "388/388 [==============================] - 0s 149us/step - loss: 0.0049 - mean_absolute_error: 0.0049 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104\n",
            "\n",
            "Epoch 00753: val_loss did not improve from 0.00505\n",
            "Epoch 754/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0071 - mean_absolute_error: 0.0071 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129\n",
            "\n",
            "Epoch 00754: val_loss did not improve from 0.00505\n",
            "Epoch 755/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0079 - mean_absolute_error: 0.0079 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087\n",
            "\n",
            "Epoch 00755: val_loss did not improve from 0.00505\n",
            "Epoch 756/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0113 - mean_absolute_error: 0.0113 - val_loss: 0.0160 - val_mean_absolute_error: 0.0160\n",
            "\n",
            "Epoch 00756: val_loss did not improve from 0.00505\n",
            "Epoch 757/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0069 - mean_absolute_error: 0.0069 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104\n",
            "\n",
            "Epoch 00757: val_loss did not improve from 0.00505\n",
            "Epoch 758/1000\n",
            "388/388 [==============================] - 0s 169us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0077 - val_mean_absolute_error: 0.0077\n",
            "\n",
            "Epoch 00758: val_loss did not improve from 0.00505\n",
            "Epoch 759/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087\n",
            "\n",
            "Epoch 00759: val_loss did not improve from 0.00505\n",
            "Epoch 760/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0078 - mean_absolute_error: 0.0078 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
            "\n",
            "Epoch 00760: val_loss did not improve from 0.00505\n",
            "Epoch 761/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0059 - mean_absolute_error: 0.0059 - val_loss: 0.0060 - val_mean_absolute_error: 0.0060\n",
            "\n",
            "Epoch 00761: val_loss did not improve from 0.00505\n",
            "Epoch 762/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0066 - val_mean_absolute_error: 0.0066\n",
            "\n",
            "Epoch 00762: val_loss did not improve from 0.00505\n",
            "Epoch 763/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0162 - val_mean_absolute_error: 0.0162\n",
            "\n",
            "Epoch 00763: val_loss did not improve from 0.00505\n",
            "Epoch 764/1000\n",
            "388/388 [==============================] - 0s 157us/step - loss: 0.0110 - mean_absolute_error: 0.0110 - val_loss: 0.0072 - val_mean_absolute_error: 0.0072\n",
            "\n",
            "Epoch 00764: val_loss did not improve from 0.00505\n",
            "Epoch 765/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0037 - mean_absolute_error: 0.0037 - val_loss: 0.0060 - val_mean_absolute_error: 0.0060\n",
            "\n",
            "Epoch 00765: val_loss did not improve from 0.00505\n",
            "Epoch 766/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0076 - val_mean_absolute_error: 0.0076\n",
            "\n",
            "Epoch 00766: val_loss did not improve from 0.00505\n",
            "Epoch 767/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0070 - val_mean_absolute_error: 0.0070\n",
            "\n",
            "Epoch 00767: val_loss did not improve from 0.00505\n",
            "Epoch 768/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0036 - mean_absolute_error: 0.0036 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069\n",
            "\n",
            "Epoch 00768: val_loss did not improve from 0.00505\n",
            "Epoch 769/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0057 - mean_absolute_error: 0.0057 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089\n",
            "\n",
            "Epoch 00769: val_loss did not improve from 0.00505\n",
            "Epoch 770/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0048 - mean_absolute_error: 0.0048 - val_loss: 0.0074 - val_mean_absolute_error: 0.0074\n",
            "\n",
            "Epoch 00770: val_loss did not improve from 0.00505\n",
            "Epoch 771/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0059 - mean_absolute_error: 0.0059 - val_loss: 0.0075 - val_mean_absolute_error: 0.0075\n",
            "\n",
            "Epoch 00771: val_loss did not improve from 0.00505\n",
            "Epoch 772/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0029 - mean_absolute_error: 0.0029 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
            "\n",
            "Epoch 00772: val_loss did not improve from 0.00505\n",
            "Epoch 773/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120\n",
            "\n",
            "Epoch 00773: val_loss did not improve from 0.00505\n",
            "Epoch 774/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0063 - mean_absolute_error: 0.0063 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103\n",
            "\n",
            "Epoch 00774: val_loss did not improve from 0.00505\n",
            "Epoch 775/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0048 - mean_absolute_error: 0.0048 - val_loss: 0.0138 - val_mean_absolute_error: 0.0138\n",
            "\n",
            "Epoch 00775: val_loss did not improve from 0.00505\n",
            "Epoch 776/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0102 - mean_absolute_error: 0.0102 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132\n",
            "\n",
            "Epoch 00776: val_loss did not improve from 0.00505\n",
            "Epoch 777/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0111 - mean_absolute_error: 0.0111 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133\n",
            "\n",
            "Epoch 00777: val_loss did not improve from 0.00505\n",
            "Epoch 778/1000\n",
            "388/388 [==============================] - 0s 124us/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137\n",
            "\n",
            "Epoch 00778: val_loss did not improve from 0.00505\n",
            "Epoch 779/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0086 - mean_absolute_error: 0.0086 - val_loss: 0.0067 - val_mean_absolute_error: 0.0067\n",
            "\n",
            "Epoch 00779: val_loss did not improve from 0.00505\n",
            "Epoch 780/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0077 - mean_absolute_error: 0.0077 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085\n",
            "\n",
            "Epoch 00780: val_loss did not improve from 0.00505\n",
            "Epoch 781/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094\n",
            "\n",
            "Epoch 00781: val_loss did not improve from 0.00505\n",
            "Epoch 782/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0064 - val_mean_absolute_error: 0.0064\n",
            "\n",
            "Epoch 00782: val_loss did not improve from 0.00505\n",
            "Epoch 783/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0067 - mean_absolute_error: 0.0067 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088\n",
            "\n",
            "Epoch 00783: val_loss did not improve from 0.00505\n",
            "Epoch 784/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0061 - val_mean_absolute_error: 0.0061\n",
            "\n",
            "Epoch 00784: val_loss did not improve from 0.00505\n",
            "Epoch 785/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0032 - mean_absolute_error: 0.0032 - val_loss: 0.0056 - val_mean_absolute_error: 0.0056\n",
            "\n",
            "Epoch 00785: val_loss did not improve from 0.00505\n",
            "Epoch 786/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0031 - mean_absolute_error: 0.0031 - val_loss: 0.0053 - val_mean_absolute_error: 0.0053\n",
            "\n",
            "Epoch 00786: val_loss did not improve from 0.00505\n",
            "Epoch 787/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0084 - val_mean_absolute_error: 0.0084\n",
            "\n",
            "Epoch 00787: val_loss did not improve from 0.00505\n",
            "Epoch 788/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0057 - mean_absolute_error: 0.0057 - val_loss: 0.0079 - val_mean_absolute_error: 0.0079\n",
            "\n",
            "Epoch 00788: val_loss did not improve from 0.00505\n",
            "Epoch 789/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0070 - mean_absolute_error: 0.0070 - val_loss: 0.0109 - val_mean_absolute_error: 0.0109\n",
            "\n",
            "Epoch 00789: val_loss did not improve from 0.00505\n",
            "Epoch 790/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0061 - mean_absolute_error: 0.0061 - val_loss: 0.0070 - val_mean_absolute_error: 0.0070\n",
            "\n",
            "Epoch 00790: val_loss did not improve from 0.00505\n",
            "Epoch 791/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0056 - mean_absolute_error: 0.0056 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128\n",
            "\n",
            "Epoch 00791: val_loss did not improve from 0.00505\n",
            "Epoch 792/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0076 - mean_absolute_error: 0.0076 - val_loss: 0.0060 - val_mean_absolute_error: 0.0060\n",
            "\n",
            "Epoch 00792: val_loss did not improve from 0.00505\n",
            "Epoch 793/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0080 - val_mean_absolute_error: 0.0080\n",
            "\n",
            "Epoch 00793: val_loss did not improve from 0.00505\n",
            "Epoch 794/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0051 - mean_absolute_error: 0.0051 - val_loss: 0.0049 - val_mean_absolute_error: 0.0049\n",
            "\n",
            "Epoch 00794: val_loss improved from 0.00505 to 0.00487, saving model to Weights-794--0.00487.hdf5\n",
            "Epoch 795/1000\n",
            "388/388 [==============================] - 0s 160us/step - loss: 0.0036 - mean_absolute_error: 0.0036 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069\n",
            "\n",
            "Epoch 00795: val_loss did not improve from 0.00487\n",
            "Epoch 796/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0112 - val_mean_absolute_error: 0.0112\n",
            "\n",
            "Epoch 00796: val_loss did not improve from 0.00487\n",
            "Epoch 797/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0067 - mean_absolute_error: 0.0067 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097\n",
            "\n",
            "Epoch 00797: val_loss did not improve from 0.00487\n",
            "Epoch 798/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0069 - mean_absolute_error: 0.0069 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118\n",
            "\n",
            "Epoch 00798: val_loss did not improve from 0.00487\n",
            "Epoch 799/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0059 - mean_absolute_error: 0.0059 - val_loss: 0.0056 - val_mean_absolute_error: 0.0056\n",
            "\n",
            "Epoch 00799: val_loss did not improve from 0.00487\n",
            "Epoch 800/1000\n",
            "388/388 [==============================] - 0s 146us/step - loss: 0.0032 - mean_absolute_error: 0.0032 - val_loss: 0.0080 - val_mean_absolute_error: 0.0080\n",
            "\n",
            "Epoch 00800: val_loss did not improve from 0.00487\n",
            "Epoch 801/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0143 - val_mean_absolute_error: 0.0143\n",
            "\n",
            "Epoch 00801: val_loss did not improve from 0.00487\n",
            "Epoch 802/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0074 - mean_absolute_error: 0.0074 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082\n",
            "\n",
            "Epoch 00802: val_loss did not improve from 0.00487\n",
            "Epoch 803/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0049 - mean_absolute_error: 0.0049 - val_loss: 0.0065 - val_mean_absolute_error: 0.0065\n",
            "\n",
            "Epoch 00803: val_loss did not improve from 0.00487\n",
            "Epoch 804/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0035 - mean_absolute_error: 0.0035 - val_loss: 0.0071 - val_mean_absolute_error: 0.0071\n",
            "\n",
            "Epoch 00804: val_loss did not improve from 0.00487\n",
            "Epoch 805/1000\n",
            "388/388 [==============================] - 0s 123us/step - loss: 0.0032 - mean_absolute_error: 0.0032 - val_loss: 0.0071 - val_mean_absolute_error: 0.0071\n",
            "\n",
            "Epoch 00805: val_loss did not improve from 0.00487\n",
            "Epoch 806/1000\n",
            "388/388 [==============================] - 0s 148us/step - loss: 0.0066 - mean_absolute_error: 0.0066 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117\n",
            "\n",
            "Epoch 00806: val_loss did not improve from 0.00487\n",
            "Epoch 807/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0056 - mean_absolute_error: 0.0056 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087\n",
            "\n",
            "Epoch 00807: val_loss did not improve from 0.00487\n",
            "Epoch 808/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0049 - mean_absolute_error: 0.0049 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100\n",
            "\n",
            "Epoch 00808: val_loss did not improve from 0.00487\n",
            "Epoch 809/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0067 - val_mean_absolute_error: 0.0067\n",
            "\n",
            "Epoch 00809: val_loss did not improve from 0.00487\n",
            "Epoch 810/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096\n",
            "\n",
            "Epoch 00810: val_loss did not improve from 0.00487\n",
            "Epoch 811/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0057 - mean_absolute_error: 0.0057 - val_loss: 0.0108 - val_mean_absolute_error: 0.0108\n",
            "\n",
            "Epoch 00811: val_loss did not improve from 0.00487\n",
            "Epoch 812/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0078 - mean_absolute_error: 0.0078 - val_loss: 0.0081 - val_mean_absolute_error: 0.0081\n",
            "\n",
            "Epoch 00812: val_loss did not improve from 0.00487\n",
            "Epoch 813/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0053 - mean_absolute_error: 0.0053 - val_loss: 0.0076 - val_mean_absolute_error: 0.0076\n",
            "\n",
            "Epoch 00813: val_loss did not improve from 0.00487\n",
            "Epoch 814/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0051 - mean_absolute_error: 0.0051 - val_loss: 0.0055 - val_mean_absolute_error: 0.0055\n",
            "\n",
            "Epoch 00814: val_loss did not improve from 0.00487\n",
            "Epoch 815/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0053 - mean_absolute_error: 0.0053 - val_loss: 0.0067 - val_mean_absolute_error: 0.0067\n",
            "\n",
            "Epoch 00815: val_loss did not improve from 0.00487\n",
            "Epoch 816/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0065 - val_mean_absolute_error: 0.0065\n",
            "\n",
            "Epoch 00816: val_loss did not improve from 0.00487\n",
            "Epoch 817/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0076 - val_mean_absolute_error: 0.0076\n",
            "\n",
            "Epoch 00817: val_loss did not improve from 0.00487\n",
            "Epoch 818/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0075 - val_mean_absolute_error: 0.0075\n",
            "\n",
            "Epoch 00818: val_loss did not improve from 0.00487\n",
            "Epoch 819/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0081 - val_mean_absolute_error: 0.0081\n",
            "\n",
            "Epoch 00819: val_loss did not improve from 0.00487\n",
            "Epoch 820/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088\n",
            "\n",
            "Epoch 00820: val_loss did not improve from 0.00487\n",
            "Epoch 821/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0060 - mean_absolute_error: 0.0060 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082\n",
            "\n",
            "Epoch 00821: val_loss did not improve from 0.00487\n",
            "Epoch 822/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0048 - mean_absolute_error: 0.0048 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
            "\n",
            "Epoch 00822: val_loss did not improve from 0.00487\n",
            "Epoch 823/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0037 - mean_absolute_error: 0.0037 - val_loss: 0.0065 - val_mean_absolute_error: 0.0065\n",
            "\n",
            "Epoch 00823: val_loss did not improve from 0.00487\n",
            "Epoch 824/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0035 - mean_absolute_error: 0.0035 - val_loss: 0.0062 - val_mean_absolute_error: 0.0062\n",
            "\n",
            "Epoch 00824: val_loss did not improve from 0.00487\n",
            "Epoch 825/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0035 - mean_absolute_error: 0.0035 - val_loss: 0.0135 - val_mean_absolute_error: 0.0135\n",
            "\n",
            "Epoch 00825: val_loss did not improve from 0.00487\n",
            "Epoch 826/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0147 - mean_absolute_error: 0.0147 - val_loss: 0.0144 - val_mean_absolute_error: 0.0144\n",
            "\n",
            "Epoch 00826: val_loss did not improve from 0.00487\n",
            "Epoch 827/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0122 - mean_absolute_error: 0.0122 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094\n",
            "\n",
            "Epoch 00827: val_loss did not improve from 0.00487\n",
            "Epoch 828/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0076 - mean_absolute_error: 0.0076 - val_loss: 0.0070 - val_mean_absolute_error: 0.0070\n",
            "\n",
            "Epoch 00828: val_loss did not improve from 0.00487\n",
            "Epoch 829/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0081 - val_mean_absolute_error: 0.0081\n",
            "\n",
            "Epoch 00829: val_loss did not improve from 0.00487\n",
            "Epoch 830/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0061 - val_mean_absolute_error: 0.0061\n",
            "\n",
            "Epoch 00830: val_loss did not improve from 0.00487\n",
            "Epoch 831/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0054 - mean_absolute_error: 0.0054 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104\n",
            "\n",
            "Epoch 00831: val_loss did not improve from 0.00487\n",
            "Epoch 832/1000\n",
            "388/388 [==============================] - 0s 151us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0075 - val_mean_absolute_error: 0.0075\n",
            "\n",
            "Epoch 00832: val_loss did not improve from 0.00487\n",
            "Epoch 833/1000\n",
            "388/388 [==============================] - 0s 164us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103\n",
            "\n",
            "Epoch 00833: val_loss did not improve from 0.00487\n",
            "Epoch 834/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0147 - val_mean_absolute_error: 0.0147\n",
            "\n",
            "Epoch 00834: val_loss did not improve from 0.00487\n",
            "Epoch 835/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0163 - mean_absolute_error: 0.0163 - val_loss: 0.0162 - val_mean_absolute_error: 0.0162\n",
            "\n",
            "Epoch 00835: val_loss did not improve from 0.00487\n",
            "Epoch 836/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0141 - mean_absolute_error: 0.0141 - val_loss: 0.0154 - val_mean_absolute_error: 0.0154\n",
            "\n",
            "Epoch 00836: val_loss did not improve from 0.00487\n",
            "Epoch 837/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126\n",
            "\n",
            "Epoch 00837: val_loss did not improve from 0.00487\n",
            "Epoch 838/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0060 - mean_absolute_error: 0.0060 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089\n",
            "\n",
            "Epoch 00838: val_loss did not improve from 0.00487\n",
            "Epoch 839/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0062 - val_mean_absolute_error: 0.0062\n",
            "\n",
            "Epoch 00839: val_loss did not improve from 0.00487\n",
            "Epoch 840/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0066 - val_mean_absolute_error: 0.0066\n",
            "\n",
            "Epoch 00840: val_loss did not improve from 0.00487\n",
            "Epoch 841/1000\n",
            "388/388 [==============================] - 0s 123us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069\n",
            "\n",
            "Epoch 00841: val_loss did not improve from 0.00487\n",
            "Epoch 842/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0078 - val_mean_absolute_error: 0.0078\n",
            "\n",
            "Epoch 00842: val_loss did not improve from 0.00487\n",
            "Epoch 843/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0072 - val_mean_absolute_error: 0.0072\n",
            "\n",
            "Epoch 00843: val_loss did not improve from 0.00487\n",
            "Epoch 844/1000\n",
            "388/388 [==============================] - 0s 122us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - val_loss: 0.0064 - val_mean_absolute_error: 0.0064\n",
            "\n",
            "Epoch 00844: val_loss did not improve from 0.00487\n",
            "Epoch 845/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0060 - val_mean_absolute_error: 0.0060\n",
            "\n",
            "Epoch 00845: val_loss did not improve from 0.00487\n",
            "Epoch 846/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0025 - mean_absolute_error: 0.0025 - val_loss: 0.0065 - val_mean_absolute_error: 0.0065\n",
            "\n",
            "Epoch 00846: val_loss did not improve from 0.00487\n",
            "Epoch 847/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0056 - mean_absolute_error: 0.0056 - val_loss: 0.0067 - val_mean_absolute_error: 0.0067\n",
            "\n",
            "Epoch 00847: val_loss did not improve from 0.00487\n",
            "Epoch 848/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0036 - mean_absolute_error: 0.0036 - val_loss: 0.0086 - val_mean_absolute_error: 0.0086\n",
            "\n",
            "Epoch 00848: val_loss did not improve from 0.00487\n",
            "Epoch 849/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103\n",
            "\n",
            "Epoch 00849: val_loss did not improve from 0.00487\n",
            "Epoch 850/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0049 - mean_absolute_error: 0.0049 - val_loss: 0.0074 - val_mean_absolute_error: 0.0074\n",
            "\n",
            "Epoch 00850: val_loss did not improve from 0.00487\n",
            "Epoch 851/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0066 - val_mean_absolute_error: 0.0066\n",
            "\n",
            "Epoch 00851: val_loss did not improve from 0.00487\n",
            "Epoch 852/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0063 - val_mean_absolute_error: 0.0063\n",
            "\n",
            "Epoch 00852: val_loss did not improve from 0.00487\n",
            "Epoch 853/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0204 - val_mean_absolute_error: 0.0204\n",
            "\n",
            "Epoch 00853: val_loss did not improve from 0.00487\n",
            "Epoch 854/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0103 - mean_absolute_error: 0.0103 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125\n",
            "\n",
            "Epoch 00854: val_loss did not improve from 0.00487\n",
            "Epoch 855/1000\n",
            "388/388 [==============================] - 0s 121us/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0078 - val_mean_absolute_error: 0.0078\n",
            "\n",
            "Epoch 00855: val_loss did not improve from 0.00487\n",
            "Epoch 856/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0075 - mean_absolute_error: 0.0075 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116\n",
            "\n",
            "Epoch 00856: val_loss did not improve from 0.00487\n",
            "Epoch 857/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0071 - mean_absolute_error: 0.0071 - val_loss: 0.0078 - val_mean_absolute_error: 0.0078\n",
            "\n",
            "Epoch 00857: val_loss did not improve from 0.00487\n",
            "Epoch 858/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0055 - val_mean_absolute_error: 0.0055\n",
            "\n",
            "Epoch 00858: val_loss did not improve from 0.00487\n",
            "Epoch 859/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0076 - val_mean_absolute_error: 0.0076\n",
            "\n",
            "Epoch 00859: val_loss did not improve from 0.00487\n",
            "Epoch 860/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0052 - mean_absolute_error: 0.0052 - val_loss: 0.0112 - val_mean_absolute_error: 0.0112\n",
            "\n",
            "Epoch 00860: val_loss did not improve from 0.00487\n",
            "Epoch 861/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0064 - mean_absolute_error: 0.0064 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119\n",
            "\n",
            "Epoch 00861: val_loss did not improve from 0.00487\n",
            "Epoch 862/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069\n",
            "\n",
            "Epoch 00862: val_loss did not improve from 0.00487\n",
            "Epoch 863/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0030 - mean_absolute_error: 0.0030 - val_loss: 0.0068 - val_mean_absolute_error: 0.0068\n",
            "\n",
            "Epoch 00863: val_loss did not improve from 0.00487\n",
            "Epoch 864/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0037 - mean_absolute_error: 0.0037 - val_loss: 0.0077 - val_mean_absolute_error: 0.0077\n",
            "\n",
            "Epoch 00864: val_loss did not improve from 0.00487\n",
            "Epoch 865/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0032 - mean_absolute_error: 0.0032 - val_loss: 0.0057 - val_mean_absolute_error: 0.0057\n",
            "\n",
            "Epoch 00865: val_loss did not improve from 0.00487\n",
            "Epoch 866/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0056 - mean_absolute_error: 0.0056 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096\n",
            "\n",
            "Epoch 00866: val_loss did not improve from 0.00487\n",
            "Epoch 867/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0064 - val_mean_absolute_error: 0.0064\n",
            "\n",
            "Epoch 00867: val_loss did not improve from 0.00487\n",
            "Epoch 868/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0051 - mean_absolute_error: 0.0051 - val_loss: 0.0175 - val_mean_absolute_error: 0.0175\n",
            "\n",
            "Epoch 00868: val_loss did not improve from 0.00487\n",
            "Epoch 869/1000\n",
            "388/388 [==============================] - 0s 152us/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0072 - val_mean_absolute_error: 0.0072\n",
            "\n",
            "Epoch 00869: val_loss did not improve from 0.00487\n",
            "Epoch 870/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0033 - mean_absolute_error: 0.0033 - val_loss: 0.0053 - val_mean_absolute_error: 0.0053\n",
            "\n",
            "Epoch 00870: val_loss did not improve from 0.00487\n",
            "Epoch 871/1000\n",
            "388/388 [==============================] - 0s 125us/step - loss: 0.0030 - mean_absolute_error: 0.0030 - val_loss: 0.0058 - val_mean_absolute_error: 0.0058\n",
            "\n",
            "Epoch 00871: val_loss did not improve from 0.00487\n",
            "Epoch 872/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0060 - mean_absolute_error: 0.0060 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082\n",
            "\n",
            "Epoch 00872: val_loss did not improve from 0.00487\n",
            "Epoch 873/1000\n",
            "388/388 [==============================] - 0s 122us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0072 - val_mean_absolute_error: 0.0072\n",
            "\n",
            "Epoch 00873: val_loss did not improve from 0.00487\n",
            "Epoch 874/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0035 - mean_absolute_error: 0.0035 - val_loss: 0.0051 - val_mean_absolute_error: 0.0051\n",
            "\n",
            "Epoch 00874: val_loss did not improve from 0.00487\n",
            "Epoch 875/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097\n",
            "\n",
            "Epoch 00875: val_loss did not improve from 0.00487\n",
            "Epoch 876/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0051 - mean_absolute_error: 0.0051 - val_loss: 0.0063 - val_mean_absolute_error: 0.0063\n",
            "\n",
            "Epoch 00876: val_loss did not improve from 0.00487\n",
            "Epoch 877/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096\n",
            "\n",
            "Epoch 00877: val_loss did not improve from 0.00487\n",
            "Epoch 878/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0066 - mean_absolute_error: 0.0066 - val_loss: 0.0067 - val_mean_absolute_error: 0.0067\n",
            "\n",
            "Epoch 00878: val_loss did not improve from 0.00487\n",
            "Epoch 879/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0062 - mean_absolute_error: 0.0062 - val_loss: 0.0059 - val_mean_absolute_error: 0.0059\n",
            "\n",
            "Epoch 00879: val_loss did not improve from 0.00487\n",
            "Epoch 880/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0067 - mean_absolute_error: 0.0067 - val_loss: 0.0191 - val_mean_absolute_error: 0.0191\n",
            "\n",
            "Epoch 00880: val_loss did not improve from 0.00487\n",
            "Epoch 881/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0063 - mean_absolute_error: 0.0063 - val_loss: 0.0063 - val_mean_absolute_error: 0.0063\n",
            "\n",
            "Epoch 00881: val_loss did not improve from 0.00487\n",
            "Epoch 882/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0033 - mean_absolute_error: 0.0033 - val_loss: 0.0060 - val_mean_absolute_error: 0.0060\n",
            "\n",
            "Epoch 00882: val_loss did not improve from 0.00487\n",
            "Epoch 883/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0072 - mean_absolute_error: 0.0072 - val_loss: 0.0077 - val_mean_absolute_error: 0.0077\n",
            "\n",
            "Epoch 00883: val_loss did not improve from 0.00487\n",
            "Epoch 884/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0060 - val_mean_absolute_error: 0.0060\n",
            "\n",
            "Epoch 00884: val_loss did not improve from 0.00487\n",
            "Epoch 885/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0056 - val_mean_absolute_error: 0.0056\n",
            "\n",
            "Epoch 00885: val_loss did not improve from 0.00487\n",
            "Epoch 886/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0036 - mean_absolute_error: 0.0036 - val_loss: 0.0058 - val_mean_absolute_error: 0.0058\n",
            "\n",
            "Epoch 00886: val_loss did not improve from 0.00487\n",
            "Epoch 887/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0036 - mean_absolute_error: 0.0036 - val_loss: 0.0047 - val_mean_absolute_error: 0.0047\n",
            "\n",
            "Epoch 00887: val_loss improved from 0.00487 to 0.00467, saving model to Weights-887--0.00467.hdf5\n",
            "Epoch 888/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089\n",
            "\n",
            "Epoch 00888: val_loss did not improve from 0.00467\n",
            "Epoch 889/1000\n",
            "388/388 [==============================] - 0s 159us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0077 - val_mean_absolute_error: 0.0077\n",
            "\n",
            "Epoch 00889: val_loss did not improve from 0.00467\n",
            "Epoch 890/1000\n",
            "388/388 [==============================] - 0s 147us/step - loss: 0.0063 - mean_absolute_error: 0.0063 - val_loss: 0.0071 - val_mean_absolute_error: 0.0071\n",
            "\n",
            "Epoch 00890: val_loss did not improve from 0.00467\n",
            "Epoch 891/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0053 - mean_absolute_error: 0.0053 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088\n",
            "\n",
            "Epoch 00891: val_loss did not improve from 0.00467\n",
            "Epoch 892/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103\n",
            "\n",
            "Epoch 00892: val_loss did not improve from 0.00467\n",
            "Epoch 893/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0052 - mean_absolute_error: 0.0052 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082\n",
            "\n",
            "Epoch 00893: val_loss did not improve from 0.00467\n",
            "Epoch 894/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0065 - mean_absolute_error: 0.0065 - val_loss: 0.0061 - val_mean_absolute_error: 0.0061\n",
            "\n",
            "Epoch 00894: val_loss did not improve from 0.00467\n",
            "Epoch 895/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0070 - val_mean_absolute_error: 0.0070\n",
            "\n",
            "Epoch 00895: val_loss did not improve from 0.00467\n",
            "Epoch 896/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0049 - mean_absolute_error: 0.0049 - val_loss: 0.0062 - val_mean_absolute_error: 0.0062\n",
            "\n",
            "Epoch 00896: val_loss did not improve from 0.00467\n",
            "Epoch 897/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0066 - val_mean_absolute_error: 0.0066\n",
            "\n",
            "Epoch 00897: val_loss did not improve from 0.00467\n",
            "Epoch 898/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0033 - mean_absolute_error: 0.0033 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128\n",
            "\n",
            "Epoch 00898: val_loss did not improve from 0.00467\n",
            "Epoch 899/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0077 - mean_absolute_error: 0.0077 - val_loss: 0.0055 - val_mean_absolute_error: 0.0055\n",
            "\n",
            "Epoch 00899: val_loss did not improve from 0.00467\n",
            "Epoch 900/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0057 - mean_absolute_error: 0.0057 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104\n",
            "\n",
            "Epoch 00900: val_loss did not improve from 0.00467\n",
            "Epoch 901/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0048 - mean_absolute_error: 0.0048 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088\n",
            "\n",
            "Epoch 00901: val_loss did not improve from 0.00467\n",
            "Epoch 902/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0068 - mean_absolute_error: 0.0068 - val_loss: 0.0098 - val_mean_absolute_error: 0.0098\n",
            "\n",
            "Epoch 00902: val_loss did not improve from 0.00467\n",
            "Epoch 903/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0059 - mean_absolute_error: 0.0059 - val_loss: 0.0075 - val_mean_absolute_error: 0.0075\n",
            "\n",
            "Epoch 00903: val_loss did not improve from 0.00467\n",
            "Epoch 904/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0056 - mean_absolute_error: 0.0056 - val_loss: 0.0081 - val_mean_absolute_error: 0.0081\n",
            "\n",
            "Epoch 00904: val_loss did not improve from 0.00467\n",
            "Epoch 905/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0076 - val_mean_absolute_error: 0.0076\n",
            "\n",
            "Epoch 00905: val_loss did not improve from 0.00467\n",
            "Epoch 906/1000\n",
            "388/388 [==============================] - 0s 146us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0057 - val_mean_absolute_error: 0.0057\n",
            "\n",
            "Epoch 00906: val_loss did not improve from 0.00467\n",
            "Epoch 907/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0072 - val_mean_absolute_error: 0.0072\n",
            "\n",
            "Epoch 00907: val_loss did not improve from 0.00467\n",
            "Epoch 908/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0049 - val_mean_absolute_error: 0.0049\n",
            "\n",
            "Epoch 00908: val_loss did not improve from 0.00467\n",
            "Epoch 909/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0049 - mean_absolute_error: 0.0049 - val_loss: 0.0145 - val_mean_absolute_error: 0.0145\n",
            "\n",
            "Epoch 00909: val_loss did not improve from 0.00467\n",
            "Epoch 910/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0111 - mean_absolute_error: 0.0111 - val_loss: 0.0098 - val_mean_absolute_error: 0.0098\n",
            "\n",
            "Epoch 00910: val_loss did not improve from 0.00467\n",
            "Epoch 911/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0138 - val_mean_absolute_error: 0.0138\n",
            "\n",
            "Epoch 00911: val_loss did not improve from 0.00467\n",
            "Epoch 912/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0059 - mean_absolute_error: 0.0059 - val_loss: 0.0053 - val_mean_absolute_error: 0.0053\n",
            "\n",
            "Epoch 00912: val_loss did not improve from 0.00467\n",
            "Epoch 913/1000\n",
            "388/388 [==============================] - 0s 123us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085\n",
            "\n",
            "Epoch 00913: val_loss did not improve from 0.00467\n",
            "Epoch 914/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0075 - val_mean_absolute_error: 0.0075\n",
            "\n",
            "Epoch 00914: val_loss did not improve from 0.00467\n",
            "Epoch 915/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0072 - val_mean_absolute_error: 0.0072\n",
            "\n",
            "Epoch 00915: val_loss did not improve from 0.00467\n",
            "Epoch 916/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0058 - val_mean_absolute_error: 0.0058\n",
            "\n",
            "Epoch 00916: val_loss did not improve from 0.00467\n",
            "Epoch 917/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0033 - mean_absolute_error: 0.0033 - val_loss: 0.0072 - val_mean_absolute_error: 0.0072\n",
            "\n",
            "Epoch 00917: val_loss did not improve from 0.00467\n",
            "Epoch 918/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0037 - mean_absolute_error: 0.0037 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104\n",
            "\n",
            "Epoch 00918: val_loss did not improve from 0.00467\n",
            "Epoch 919/1000\n",
            "388/388 [==============================] - 0s 138us/step - loss: 0.0058 - mean_absolute_error: 0.0058 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102\n",
            "\n",
            "Epoch 00919: val_loss did not improve from 0.00467\n",
            "Epoch 920/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
            "\n",
            "Epoch 00920: val_loss did not improve from 0.00467\n",
            "Epoch 921/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0057 - val_mean_absolute_error: 0.0057\n",
            "\n",
            "Epoch 00921: val_loss did not improve from 0.00467\n",
            "Epoch 922/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0036 - mean_absolute_error: 0.0036 - val_loss: 0.0052 - val_mean_absolute_error: 0.0052\n",
            "\n",
            "Epoch 00922: val_loss did not improve from 0.00467\n",
            "Epoch 923/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0029 - mean_absolute_error: 0.0029 - val_loss: 0.0080 - val_mean_absolute_error: 0.0080\n",
            "\n",
            "Epoch 00923: val_loss did not improve from 0.00467\n",
            "Epoch 924/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0071 - mean_absolute_error: 0.0071 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132\n",
            "\n",
            "Epoch 00924: val_loss did not improve from 0.00467\n",
            "Epoch 925/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0085 - mean_absolute_error: 0.0085 - val_loss: 0.0109 - val_mean_absolute_error: 0.0109\n",
            "\n",
            "Epoch 00925: val_loss did not improve from 0.00467\n",
            "Epoch 926/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0108 - val_mean_absolute_error: 0.0108\n",
            "\n",
            "Epoch 00926: val_loss did not improve from 0.00467\n",
            "Epoch 927/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0052 - mean_absolute_error: 0.0052 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106\n",
            "\n",
            "Epoch 00927: val_loss did not improve from 0.00467\n",
            "Epoch 928/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0060 - mean_absolute_error: 0.0060 - val_loss: 0.0141 - val_mean_absolute_error: 0.0141\n",
            "\n",
            "Epoch 00928: val_loss did not improve from 0.00467\n",
            "Epoch 929/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0073 - mean_absolute_error: 0.0073 - val_loss: 0.0061 - val_mean_absolute_error: 0.0061\n",
            "\n",
            "Epoch 00929: val_loss did not improve from 0.00467\n",
            "Epoch 930/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0081 - val_mean_absolute_error: 0.0081\n",
            "\n",
            "Epoch 00930: val_loss did not improve from 0.00467\n",
            "Epoch 931/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0057 - mean_absolute_error: 0.0057 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082\n",
            "\n",
            "Epoch 00931: val_loss did not improve from 0.00467\n",
            "Epoch 932/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0035 - mean_absolute_error: 0.0035 - val_loss: 0.0064 - val_mean_absolute_error: 0.0064\n",
            "\n",
            "Epoch 00932: val_loss did not improve from 0.00467\n",
            "Epoch 933/1000\n",
            "388/388 [==============================] - 0s 204us/step - loss: 0.0032 - mean_absolute_error: 0.0032 - val_loss: 0.0057 - val_mean_absolute_error: 0.0057\n",
            "\n",
            "Epoch 00933: val_loss did not improve from 0.00467\n",
            "Epoch 934/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0029 - mean_absolute_error: 0.0029 - val_loss: 0.0062 - val_mean_absolute_error: 0.0062\n",
            "\n",
            "Epoch 00934: val_loss did not improve from 0.00467\n",
            "Epoch 935/1000\n",
            "388/388 [==============================] - 0s 145us/step - loss: 0.0030 - mean_absolute_error: 0.0030 - val_loss: 0.0081 - val_mean_absolute_error: 0.0081\n",
            "\n",
            "Epoch 00935: val_loss did not improve from 0.00467\n",
            "Epoch 936/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0061 - mean_absolute_error: 0.0061 - val_loss: 0.0182 - val_mean_absolute_error: 0.0182\n",
            "\n",
            "Epoch 00936: val_loss did not improve from 0.00467\n",
            "Epoch 937/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0099 - mean_absolute_error: 0.0099 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122\n",
            "\n",
            "Epoch 00937: val_loss did not improve from 0.00467\n",
            "Epoch 938/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0064 - val_mean_absolute_error: 0.0064\n",
            "\n",
            "Epoch 00938: val_loss did not improve from 0.00467\n",
            "Epoch 939/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0037 - mean_absolute_error: 0.0037 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097\n",
            "\n",
            "Epoch 00939: val_loss did not improve from 0.00467\n",
            "Epoch 940/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0055 - mean_absolute_error: 0.0055 - val_loss: 0.0157 - val_mean_absolute_error: 0.0157\n",
            "\n",
            "Epoch 00940: val_loss did not improve from 0.00467\n",
            "Epoch 941/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120\n",
            "\n",
            "Epoch 00941: val_loss did not improve from 0.00467\n",
            "Epoch 942/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0063 - mean_absolute_error: 0.0063 - val_loss: 0.0058 - val_mean_absolute_error: 0.0058\n",
            "\n",
            "Epoch 00942: val_loss did not improve from 0.00467\n",
            "Epoch 943/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0063 - val_mean_absolute_error: 0.0063\n",
            "\n",
            "Epoch 00943: val_loss did not improve from 0.00467\n",
            "Epoch 944/1000\n",
            "388/388 [==============================] - 0s 150us/step - loss: 0.0030 - mean_absolute_error: 0.0030 - val_loss: 0.0061 - val_mean_absolute_error: 0.0061\n",
            "\n",
            "Epoch 00944: val_loss did not improve from 0.00467\n",
            "Epoch 945/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087\n",
            "\n",
            "Epoch 00945: val_loss did not improve from 0.00467\n",
            "Epoch 946/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0056 - mean_absolute_error: 0.0056 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103\n",
            "\n",
            "Epoch 00946: val_loss did not improve from 0.00467\n",
            "Epoch 947/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0061 - mean_absolute_error: 0.0061 - val_loss: 0.0078 - val_mean_absolute_error: 0.0078\n",
            "\n",
            "Epoch 00947: val_loss did not improve from 0.00467\n",
            "Epoch 948/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0037 - mean_absolute_error: 0.0037 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122\n",
            "\n",
            "Epoch 00948: val_loss did not improve from 0.00467\n",
            "Epoch 949/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0058 - val_mean_absolute_error: 0.0058\n",
            "\n",
            "Epoch 00949: val_loss did not improve from 0.00467\n",
            "Epoch 950/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0109 - val_mean_absolute_error: 0.0109\n",
            "\n",
            "Epoch 00950: val_loss did not improve from 0.00467\n",
            "Epoch 951/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0073 - mean_absolute_error: 0.0073 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100\n",
            "\n",
            "Epoch 00951: val_loss did not improve from 0.00467\n",
            "Epoch 952/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0056 - mean_absolute_error: 0.0056 - val_loss: 0.0095 - val_mean_absolute_error: 0.0095\n",
            "\n",
            "Epoch 00952: val_loss did not improve from 0.00467\n",
            "Epoch 953/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0065 - mean_absolute_error: 0.0065 - val_loss: 0.0066 - val_mean_absolute_error: 0.0066\n",
            "\n",
            "Epoch 00953: val_loss did not improve from 0.00467\n",
            "Epoch 954/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0053 - mean_absolute_error: 0.0053 - val_loss: 0.0061 - val_mean_absolute_error: 0.0061\n",
            "\n",
            "Epoch 00954: val_loss did not improve from 0.00467\n",
            "Epoch 955/1000\n",
            "388/388 [==============================] - 0s 148us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089\n",
            "\n",
            "Epoch 00955: val_loss did not improve from 0.00467\n",
            "Epoch 956/1000\n",
            "388/388 [==============================] - 0s 143us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0084 - val_mean_absolute_error: 0.0084\n",
            "\n",
            "Epoch 00956: val_loss did not improve from 0.00467\n",
            "Epoch 957/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0051 - mean_absolute_error: 0.0051 - val_loss: 0.0078 - val_mean_absolute_error: 0.0078\n",
            "\n",
            "Epoch 00957: val_loss did not improve from 0.00467\n",
            "Epoch 958/1000\n",
            "388/388 [==============================] - 0s 215us/step - loss: 0.0056 - mean_absolute_error: 0.0056 - val_loss: 0.0145 - val_mean_absolute_error: 0.0145\n",
            "\n",
            "Epoch 00958: val_loss did not improve from 0.00467\n",
            "Epoch 959/1000\n",
            "388/388 [==============================] - 0s 162us/step - loss: 0.0065 - mean_absolute_error: 0.0065 - val_loss: 0.0079 - val_mean_absolute_error: 0.0079\n",
            "\n",
            "Epoch 00959: val_loss did not improve from 0.00467\n",
            "Epoch 960/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0035 - mean_absolute_error: 0.0035 - val_loss: 0.0081 - val_mean_absolute_error: 0.0081\n",
            "\n",
            "Epoch 00960: val_loss did not improve from 0.00467\n",
            "Epoch 961/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0054 - mean_absolute_error: 0.0054 - val_loss: 0.0072 - val_mean_absolute_error: 0.0072\n",
            "\n",
            "Epoch 00961: val_loss did not improve from 0.00467\n",
            "Epoch 962/1000\n",
            "388/388 [==============================] - 0s 148us/step - loss: 0.0057 - mean_absolute_error: 0.0057 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118\n",
            "\n",
            "Epoch 00962: val_loss did not improve from 0.00467\n",
            "Epoch 963/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0077 - mean_absolute_error: 0.0077 - val_loss: 0.0080 - val_mean_absolute_error: 0.0080\n",
            "\n",
            "Epoch 00963: val_loss did not improve from 0.00467\n",
            "Epoch 964/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0037 - mean_absolute_error: 0.0037 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116\n",
            "\n",
            "Epoch 00964: val_loss did not improve from 0.00467\n",
            "Epoch 965/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0074 - mean_absolute_error: 0.0074 - val_loss: 0.0075 - val_mean_absolute_error: 0.0075\n",
            "\n",
            "Epoch 00965: val_loss did not improve from 0.00467\n",
            "Epoch 966/1000\n",
            "388/388 [==============================] - 0s 124us/step - loss: 0.0054 - mean_absolute_error: 0.0054 - val_loss: 0.0192 - val_mean_absolute_error: 0.0192\n",
            "\n",
            "Epoch 00966: val_loss did not improve from 0.00467\n",
            "Epoch 967/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0093 - mean_absolute_error: 0.0093 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120\n",
            "\n",
            "Epoch 00967: val_loss did not improve from 0.00467\n",
            "Epoch 968/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0074 - mean_absolute_error: 0.0074 - val_loss: 0.0072 - val_mean_absolute_error: 0.0072\n",
            "\n",
            "Epoch 00968: val_loss did not improve from 0.00467\n",
            "Epoch 969/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0138 - val_mean_absolute_error: 0.0138\n",
            "\n",
            "Epoch 00969: val_loss did not improve from 0.00467\n",
            "Epoch 970/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0108 - mean_absolute_error: 0.0108 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106\n",
            "\n",
            "Epoch 00970: val_loss did not improve from 0.00467\n",
            "Epoch 971/1000\n",
            "388/388 [==============================] - 0s 145us/step - loss: 0.0070 - mean_absolute_error: 0.0070 - val_loss: 0.0074 - val_mean_absolute_error: 0.0074\n",
            "\n",
            "Epoch 00971: val_loss did not improve from 0.00467\n",
            "Epoch 972/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0061 - mean_absolute_error: 0.0061 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104\n",
            "\n",
            "Epoch 00972: val_loss did not improve from 0.00467\n",
            "Epoch 973/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0075 - val_mean_absolute_error: 0.0075\n",
            "\n",
            "Epoch 00973: val_loss did not improve from 0.00467\n",
            "Epoch 974/1000\n",
            "388/388 [==============================] - 0s 127us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0070 - val_mean_absolute_error: 0.0070\n",
            "\n",
            "Epoch 00974: val_loss did not improve from 0.00467\n",
            "Epoch 975/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - val_loss: 0.0108 - val_mean_absolute_error: 0.0108\n",
            "\n",
            "Epoch 00975: val_loss did not improve from 0.00467\n",
            "Epoch 976/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0052 - mean_absolute_error: 0.0052 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069\n",
            "\n",
            "Epoch 00976: val_loss did not improve from 0.00467\n",
            "Epoch 977/1000\n",
            "388/388 [==============================] - 0s 144us/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118\n",
            "\n",
            "Epoch 00977: val_loss did not improve from 0.00467\n",
            "Epoch 978/1000\n",
            "388/388 [==============================] - 0s 140us/step - loss: 0.0057 - mean_absolute_error: 0.0057 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121\n",
            "\n",
            "Epoch 00978: val_loss did not improve from 0.00467\n",
            "Epoch 979/1000\n",
            "388/388 [==============================] - 0s 142us/step - loss: 0.0069 - mean_absolute_error: 0.0069 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099\n",
            "\n",
            "Epoch 00979: val_loss did not improve from 0.00467\n",
            "Epoch 980/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0076 - mean_absolute_error: 0.0076 - val_loss: 0.0080 - val_mean_absolute_error: 0.0080\n",
            "\n",
            "Epoch 00980: val_loss did not improve from 0.00467\n",
            "Epoch 981/1000\n",
            "388/388 [==============================] - 0s 136us/step - loss: 0.0070 - mean_absolute_error: 0.0070 - val_loss: 0.0084 - val_mean_absolute_error: 0.0084\n",
            "\n",
            "Epoch 00981: val_loss did not improve from 0.00467\n",
            "Epoch 982/1000\n",
            "388/388 [==============================] - 0s 137us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0073 - val_mean_absolute_error: 0.0073\n",
            "\n",
            "Epoch 00982: val_loss did not improve from 0.00467\n",
            "Epoch 983/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0071 - mean_absolute_error: 0.0071 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082\n",
            "\n",
            "Epoch 00983: val_loss did not improve from 0.00467\n",
            "Epoch 984/1000\n",
            "388/388 [==============================] - 0s 130us/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0107 - val_mean_absolute_error: 0.0107\n",
            "\n",
            "Epoch 00984: val_loss did not improve from 0.00467\n",
            "Epoch 985/1000\n",
            "388/388 [==============================] - 0s 126us/step - loss: 0.0045 - mean_absolute_error: 0.0045 - val_loss: 0.0059 - val_mean_absolute_error: 0.0059\n",
            "\n",
            "Epoch 00985: val_loss did not improve from 0.00467\n",
            "Epoch 986/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0033 - mean_absolute_error: 0.0033 - val_loss: 0.0077 - val_mean_absolute_error: 0.0077\n",
            "\n",
            "Epoch 00986: val_loss did not improve from 0.00467\n",
            "Epoch 987/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0074 - mean_absolute_error: 0.0074 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132\n",
            "\n",
            "Epoch 00987: val_loss did not improve from 0.00467\n",
            "Epoch 988/1000\n",
            "388/388 [==============================] - 0s 129us/step - loss: 0.0064 - mean_absolute_error: 0.0064 - val_loss: 0.0147 - val_mean_absolute_error: 0.0147\n",
            "\n",
            "Epoch 00988: val_loss did not improve from 0.00467\n",
            "Epoch 989/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0133 - mean_absolute_error: 0.0133 - val_loss: 0.0162 - val_mean_absolute_error: 0.0162\n",
            "\n",
            "Epoch 00989: val_loss did not improve from 0.00467\n",
            "Epoch 990/1000\n",
            "388/388 [==============================] - 0s 128us/step - loss: 0.0116 - mean_absolute_error: 0.0116 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106\n",
            "\n",
            "Epoch 00990: val_loss did not improve from 0.00467\n",
            "Epoch 991/1000\n",
            "388/388 [==============================] - 0s 132us/step - loss: 0.0087 - mean_absolute_error: 0.0087 - val_loss: 0.0112 - val_mean_absolute_error: 0.0112\n",
            "\n",
            "Epoch 00991: val_loss did not improve from 0.00467\n",
            "Epoch 992/1000\n",
            "388/388 [==============================] - 0s 131us/step - loss: 0.0075 - mean_absolute_error: 0.0075 - val_loss: 0.0077 - val_mean_absolute_error: 0.0077\n",
            "\n",
            "Epoch 00992: val_loss did not improve from 0.00467\n",
            "Epoch 993/1000\n",
            "388/388 [==============================] - 0s 135us/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089\n",
            "\n",
            "Epoch 00993: val_loss did not improve from 0.00467\n",
            "Epoch 994/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0062 - mean_absolute_error: 0.0062 - val_loss: 0.0072 - val_mean_absolute_error: 0.0072\n",
            "\n",
            "Epoch 00994: val_loss did not improve from 0.00467\n",
            "Epoch 995/1000\n",
            "388/388 [==============================] - 0s 163us/step - loss: 0.0035 - mean_absolute_error: 0.0035 - val_loss: 0.0075 - val_mean_absolute_error: 0.0075\n",
            "\n",
            "Epoch 00995: val_loss did not improve from 0.00467\n",
            "Epoch 996/1000\n",
            "388/388 [==============================] - 0s 152us/step - loss: 0.0036 - mean_absolute_error: 0.0036 - val_loss: 0.0068 - val_mean_absolute_error: 0.0068\n",
            "\n",
            "Epoch 00996: val_loss did not improve from 0.00467\n",
            "Epoch 997/1000\n",
            "388/388 [==============================] - 0s 139us/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0083 - val_mean_absolute_error: 0.0083\n",
            "\n",
            "Epoch 00997: val_loss did not improve from 0.00467\n",
            "Epoch 998/1000\n",
            "388/388 [==============================] - 0s 133us/step - loss: 0.0053 - mean_absolute_error: 0.0053 - val_loss: 0.0072 - val_mean_absolute_error: 0.0072\n",
            "\n",
            "Epoch 00998: val_loss did not improve from 0.00467\n",
            "Epoch 999/1000\n",
            "388/388 [==============================] - 0s 141us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0079 - val_mean_absolute_error: 0.0079\n",
            "\n",
            "Epoch 00999: val_loss did not improve from 0.00467\n",
            "Epoch 1000/1000\n",
            "388/388 [==============================] - 0s 134us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0062 - val_mean_absolute_error: 0.0062\n",
            "\n",
            "Epoch 01000: val_loss did not improve from 0.00467\n",
            "112\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxiRkwcIYY0xCFiyMMcYkZMHCGGNMQv8PwqJHbmNzEUYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-qzJFTgGsWh",
        "colab_type": "code",
        "outputId": "fdbd6c46-e757-4780-ef6b-e9efeab66884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataframealluser1.csv      Weights-011--0.02130.hdf5  Weights-089--0.00800.hdf5\n",
            "Linear.png                 Weights-013--0.02403.hdf5  Weights-113--0.00701.hdf5\n",
            "MSEDNN.png                 Weights-014--0.02177.hdf5  Weights-113--0.00742.hdf5\n",
            "Poly.png                   Weights-014--0.02933.hdf5  Weights-114--0.01140.hdf5\n",
            "RBF.png                    Weights-016--0.02436.hdf5  Weights-170--0.01139.hdf5\n",
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/               Weights-017--0.01825.hdf5  Weights-176--0.00691.hdf5\n",
            "Weights-001--0.22423.hdf5  Weights-018--0.01816.hdf5  Weights-201--0.00710.hdf5\n",
            "Weights-001--0.22889.hdf5  Weights-018--0.01888.hdf5  Weights-216--0.00696.hdf5\n",
            "Weights-001--0.23188.hdf5  Weights-019--0.01415.hdf5  Weights-217--0.00865.hdf5\n",
            "Weights-002--0.09334.hdf5  Weights-019--0.02337.hdf5  Weights-225--0.00690.hdf5\n",
            "Weights-002--0.13720.hdf5  Weights-020--0.01492.hdf5  Weights-226--0.00657.hdf5\n",
            "Weights-002--0.17912.hdf5  Weights-029--0.01158.hdf5  Weights-299--0.00690.hdf5\n",
            "Weights-003--0.06088.hdf5  Weights-030--0.01963.hdf5  Weights-304--0.00648.hdf5\n",
            "Weights-003--0.07084.hdf5  Weights-031--0.01781.hdf5  Weights-308--0.00611.hdf5\n",
            "Weights-003--0.07496.hdf5  Weights-032--0.01492.hdf5  Weights-414--0.00635.hdf5\n",
            "Weights-004--0.04912.hdf5  Weights-034--0.01473.hdf5  Weights-432--0.00616.hdf5\n",
            "Weights-004--0.06436.hdf5  Weights-036--0.01463.hdf5  Weights-438--0.00593.hdf5\n",
            "Weights-005--0.03485.hdf5  Weights-038--0.01255.hdf5  Weights-455--0.00550.hdf5\n",
            "Weights-005--0.04119.hdf5  Weights-049--0.01071.hdf5  Weights-545--0.00602.hdf5\n",
            "Weights-005--0.04949.hdf5  Weights-052--0.01076.hdf5  Weights-562--0.00537.hdf5\n",
            "Weights-006--0.02920.hdf5  Weights-069--0.01456.hdf5  Weights-577--0.00844.hdf5\n",
            "Weights-008--0.03540.hdf5  Weights-070--0.00871.hdf5  Weights-633--0.00536.hdf5\n",
            "Weights-009--0.02765.hdf5  Weights-072--0.01444.hdf5  Weights-637--0.00527.hdf5\n",
            "Weights-009--0.03882.hdf5  Weights-079--0.01152.hdf5  Weights-695--0.00505.hdf5\n",
            "Weights-010--0.02233.hdf5  Weights-081--0.00859.hdf5  Weights-794--0.00487.hdf5\n",
            "Weights-010--0.02936.hdf5  Weights-085--0.01013.hdf5  Weights-887--0.00467.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY9tApSHIJw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####################Plotting graph for socre of different models###############################\n",
        "\n",
        "\n",
        "index = np.arange(len(model))\n",
        "plt.bar(index,accuracy,alpha=1,color='blue')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Different Machine Learning Models')\n",
        "plt.xticks(index,model)\n",
        "plt.title(\"Models Comparison\")\n",
        "plt.show()\n",
        "#######feautre correlation visualizing using seaborn###############"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}